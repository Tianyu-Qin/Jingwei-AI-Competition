{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torch.utils.model_zoo as model_zoo\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://raw.githubusercontent.com/delta-onera/delta_tb/master/deltatb/networks/net_segnet_bn_relu.py\n",
    "class SegNet_BN_ReLU(nn.Module):\n",
    "    # Unet network\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal(m.weight.data)\n",
    "     \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SegNet_BN_ReLU, self).__init__()\n",
    " \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    " \n",
    "        self.pool = nn.MaxPool2d(2, return_indices=True)\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "         \n",
    "        self.conv1_1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
    "        self.conv1_1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv1_2_bn = nn.BatchNorm2d(64)\n",
    "         \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2_1_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv2_2_bn = nn.BatchNorm2d(128)\n",
    "         \n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3_1_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_2_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_3_bn = nn.BatchNorm2d(256)\n",
    "         \n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv4_1_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_2_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_3_bn = nn.BatchNorm2d(512)\n",
    "         \n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_1_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_2_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_3_bn = nn.BatchNorm2d(512)\n",
    "         \n",
    "        self.conv5_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_3_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_2_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv5_1_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv5_1_D_bn = nn.BatchNorm2d(512)\n",
    "         \n",
    "        self.conv4_3_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_3_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_2_D = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_2_D_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4_1_D = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.conv4_1_D_bn = nn.BatchNorm2d(256)\n",
    "         \n",
    "        self.conv3_3_D = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_3_D_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_2_D = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_2_D_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_1_D = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv3_1_D_bn = nn.BatchNorm2d(128)\n",
    "         \n",
    "        self.conv2_2_D = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv2_2_D_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_1_D = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv2_1_D_bn = nn.BatchNorm2d(64)\n",
    "         \n",
    "        self.conv1_2_D = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv1_2_D_bn = nn.BatchNorm2d(64)\n",
    "        self.conv1_1_D = nn.Conv2d(64, out_channels, 3, padding=1)\n",
    "         \n",
    "        self.apply(self.weight_init)\n",
    "         \n",
    "    def forward(self, x):\n",
    "        # Encoder block 1\n",
    "        x =F.avg_pool2d(x,4)\n",
    "        #print(x.shape)\n",
    "        x = self.conv1_1_bn(F.relu(self.conv1_1(x)))\n",
    "        x1 = self.conv1_2_bn(F.relu(self.conv1_2(x)))\n",
    "        size1 = x.size()\n",
    "        x, mask1 = self.pool(x1)\n",
    "         \n",
    "        # Encoder block 2\n",
    "        x = self.conv2_1_bn(F.relu(self.conv2_1(x)))\n",
    "        #x = self.drop2_1(x)\n",
    "        x2 = self.conv2_2_bn(F.relu(self.conv2_2(x)))\n",
    "        size2 = x.size()\n",
    "        x, mask2 = self.pool(x2)\n",
    "         \n",
    "        # Encoder block 3\n",
    "        x = self.conv3_1_bn(F.relu(self.conv3_1(x)))\n",
    "        x = self.conv3_2_bn(F.relu(self.conv3_2(x)))\n",
    "        x3 = self.conv3_3_bn(F.relu(self.conv3_3(x)))\n",
    "        size3 = x.size()\n",
    "        x, mask3 = self.pool(x3)\n",
    "         \n",
    "        # Encoder block 4\n",
    "        x = self.conv4_1_bn(F.relu(self.conv4_1(x)))\n",
    "        x = self.conv4_2_bn(F.relu(self.conv4_2(x)))\n",
    "        x4 = self.conv4_3_bn(F.relu(self.conv4_3(x)))\n",
    "        size4 = x.size()\n",
    "        x, mask4 = self.pool(x4)\n",
    "         \n",
    "        # Encoder block 5\n",
    "        x = self.conv5_1_bn(F.relu(self.conv5_1(x)))\n",
    "        x = self.conv5_2_bn(F.relu(self.conv5_2(x)))\n",
    "        x = self.conv5_3_bn(F.relu(self.conv5_3(x)))\n",
    "        size5 = x.size()\n",
    "        x, mask5 = self.pool(x)\n",
    "         \n",
    "        # Decoder block 5\n",
    "        x = self.unpool(x, mask5, output_size = size5)\n",
    "        x = self.conv5_3_D_bn(F.relu(self.conv5_3_D(x)))\n",
    "        x = self.conv5_2_D_bn(F.relu(self.conv5_2_D(x)))\n",
    "        x = self.conv5_1_D_bn(F.relu(self.conv5_1_D(x)))\n",
    "         \n",
    "        # Decoder block 4\n",
    "        x = self.unpool(x, mask4, output_size = size4)\n",
    "        x = self.conv4_3_D_bn(F.relu(self.conv4_3_D(x)))\n",
    "        x = self.conv4_2_D_bn(F.relu(self.conv4_2_D(x)))\n",
    "        x = self.conv4_1_D_bn(F.relu(self.conv4_1_D(x)))\n",
    "         \n",
    "        # Decoder block 3\n",
    "        x = self.unpool(x, mask3, output_size = size3)\n",
    "        x = self.conv3_3_D_bn(F.relu(self.conv3_3_D(x)))\n",
    "        x = self.conv3_2_D_bn(F.relu(self.conv3_2_D(x)))\n",
    "        x = self.conv3_1_D_bn(F.relu(self.conv3_1_D(x)))\n",
    "         \n",
    "        # Decoder block 2\n",
    "        x = self.unpool(x, mask2, output_size = size2)\n",
    "        x = self.conv2_2_D_bn(F.relu(self.conv2_2_D(x)))\n",
    "        x = self.conv2_1_D_bn(F.relu(self.conv2_1_D(x)))\n",
    "         \n",
    "        # Decoder block 1\n",
    "        x = self.unpool(x, mask1, output_size = size1)\n",
    "        x = self.conv1_2_D_bn(F.relu(self.conv1_2_D(x)))\n",
    "        x = self.conv1_1_D(x)\n",
    "#         return x\n",
    "        #print(x.shape)\n",
    "        return F.interpolate(x,mode='bilinear',scale_factor=4) # upsample, but we don't need this anymore\n",
    "\n",
    " \n",
    "    def load_pretrained_weights(self):\n",
    " \n",
    "        #vgg16_weights = model_zoo.load_url(\"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\")\n",
    "        vgg16_weights=models.vgg16_bn(True).state_dict()\n",
    "        count_vgg = 0\n",
    "        count_this = 0\n",
    " \n",
    "        vggkeys = list(vgg16_weights.keys())\n",
    "        thiskeys  = list(self.state_dict().keys())\n",
    " \n",
    "        corresp_map = []\n",
    " \n",
    "        while(True):\n",
    "            vggkey = vggkeys[count_vgg]\n",
    "            thiskey = thiskeys[count_this]\n",
    " \n",
    "            if \"classifier\" in vggkey:\n",
    "                break\n",
    "             \n",
    "            while vggkey.split(\".\")[-1] not in thiskey:\n",
    "                count_this += 1\n",
    "                thiskey = thiskeys[count_this]\n",
    " \n",
    " \n",
    "            corresp_map.append([vggkey, thiskey])\n",
    "            count_vgg+=1\n",
    "            count_this += 1\n",
    " \n",
    "        mapped_weights = self.state_dict()\n",
    "        for k_vgg, k_segnet in corresp_map:\n",
    "            if (self.in_channels != 3) and \"features\" in k_vgg and \"conv1_1.\" not in k_segnet:\n",
    "                mapped_weights[k_segnet] = vgg16_weights[k_vgg]\n",
    "            elif (self.in_channels == 3) and \"features\" in k_vgg:\n",
    "                mapped_weights[k_segnet] = vgg16_weights[k_vgg]\n",
    " \n",
    "        try:\n",
    "            self.load_state_dict(mapped_weights)\n",
    "            print(\"Loaded VGG-16 weights in Segnet !\")\n",
    "        except:\n",
    "            print(\"Error VGG-16 weights in Segnet !\")\n",
    "            raise\n",
    "     \n",
    "    def load_from_filename(self, model_path):\n",
    "        \"\"\"Load weights from filename.\"\"\"\n",
    "        th = torch.load(model_path)  # load the weigths\n",
    "        self.load_state_dict(th)\n",
    " \n",
    " \n",
    "def segnet_bn_relu(in_channels, out_channels, pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = SegNet_BN_ReLU(in_channels, out_channels)\n",
    "    if pretrained:\n",
    "        model.load_pretrained_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegNet_BN_ReLU(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
      "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_3_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_3_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_2_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_2_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5_1_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5_1_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_3_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_3_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_2_D): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_2_D_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4_1_D): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_1_D_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_3_D): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_3_D_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_2_D): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_2_D_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3_1_D): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_1_D_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_2_D): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_2_D_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2_1_D): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_1_D_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1_2_D): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_2_D_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1_1_D): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "torch.Size([1, 4, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "net=segnet_bn_relu(3,4,False)\n",
    "print(net)\n",
    "x=torch.rand((1,3,512,512))\n",
    "print(net.forward(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from farmdataset import FarmDataset\n",
    "from segnet import segnet_bn_relu as Unet\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #print(target.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print('output size',output.size(),output)\n",
    " \n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = nn.NLLLoss2d(weight=torch.Tensor([0.1,0.5,0.5,0.2]).to('cuda'))(output,target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    " \n",
    "        #time.sleep(0.6)#make gpu sleep\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    end_time = time.time()\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    print('Training Time: {}   Learning Rate: {}'.format(end_time - start_time, curr_lr))\n",
    "    if epoch%2 == 0:\n",
    "        imgd = output.detach()[0,:,:,:].cpu()\n",
    "        img = torch.argmax(imgd,0).byte().numpy()\n",
    "        imgx = Image.fromarray(img).convert('L')\n",
    "        imgxx = Image.fromarray(target.detach()[0,:,:].cpu().byte().numpy()*255).convert('L')\n",
    "        imgx.save(\"./tmp/predict{}.bmp\".format(epoch))\n",
    "        imgxx.save('./tmp/real{}.bmp'.format(epoch))\n",
    " \n",
    "def test(model, device, testdataset, issave=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    evalid = [i+7 for i in range(0,2100,15)]\n",
    "    maxbatch = len(evalid)\n",
    "    with torch.no_grad():\n",
    "        for idx in evalid:\n",
    "            data, target = testdataset[idx]\n",
    "            data, target = data.unsqueeze(0).to(device), target.unsqueeze(0).to(device)\n",
    "            target = target[:,:1472,:1472]\n",
    "            output = model(data[:,:,:1472,:1472])\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            loss = nn.NLLLoss2d().to('cuda')(output,target)\n",
    "            test_loss += loss\n",
    "             \n",
    "            r = torch.argmax(output[0],0).byte()\n",
    "  \n",
    "            tg = target.byte().squeeze(0)\n",
    "            tmp = 0\n",
    "            count = 0\n",
    "            for i in range(1,4):\n",
    "                mp = r == i\n",
    "                tr = tg == i\n",
    "                tp = mp*tr == 1\n",
    "                t = (mp+tr-tp).sum().item()\n",
    "                if t == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp += tp.sum().item()/t\n",
    "                    count += 1\n",
    "            if count>0:\n",
    "                correct += tmp/count\n",
    "            \n",
    "             \n",
    "            if issave:\n",
    "                Image.fromarray(r.cpu().numpy()).save('predict.png')\n",
    "                Image.fromarray(tg.cpu().numpy()).save('target.png')\n",
    "                input()\n",
    "                 \n",
    "    print('Test Loss is {:.6f}, mean precision is: {:.4f}%'.format(test_loss/maxbatch,correct))\n",
    " \n",
    " \n",
    "def main():\n",
    "    # Training settings\n",
    "#     parser = argparse.ArgumentParser(description='Scratch segmentation Example')\n",
    "#     parser.add_argument('--batch-size', type=int, default=8, metavar='N',\n",
    "#                         help='input batch size for training (default: 64)')\n",
    "#     parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "#                         help='input batch size for testing (default: 1000)')\n",
    "#     parser.add_argument('--epochs', type=int, default=30, metavar='N',\n",
    "#                         help='number of epochs to train (default: 10)')\n",
    "#     parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "#                         help='learning rate (default: 0.01)')\n",
    "#     parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                         help='SGD momentum (default: 0.5)')\n",
    "#     parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                         help='disables CUDA training')\n",
    "#     parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                         help='random seed (default: 1)')\n",
    "#     parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                         help='how many batches to wait before logging training status')\n",
    "#     args = parser.parse_args()\n",
    "    use_cuda = torch.cuda.is_available()\n",
    " \n",
    "    torch.manual_seed(1)\n",
    " \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print('my device is :',device)\n",
    " \n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(FarmDataset(istrain=True), batch_size = 5, shuffle=True, drop_last=True, **kwargs)\n",
    "     \n",
    "    startepoch = 0\n",
    "    model = torch.load('./tmp/model{}'.format(startepoch))  if startepoch else Unet(3,4).to(device) \n",
    "    epochs = 22\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    " \n",
    "    for epoch in range(startepoch, epochs + 1):\n",
    "        if epoch in [10, 16]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        if epoch % 2 == 0:\n",
    "            print(epoch)\n",
    "            test(model, device, FarmDataset(istrain = True, isaug = False), issave = False)\n",
    "            torch.save(model,'./tmp/model{}'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my device is : cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makisechris/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py:216: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.\n",
      "  warnings.warn(\"NLLLoss2d has been deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/19646 (0%)]\tLoss: 1.394002\n",
      "Train Epoch: 0 [500/19646 (3%)]\tLoss: 1.483058\n",
      "Train Epoch: 0 [1000/19646 (5%)]\tLoss: 1.342247\n",
      "Train Epoch: 0 [1500/19646 (8%)]\tLoss: 1.459571\n",
      "Train Epoch: 0 [2000/19646 (10%)]\tLoss: 1.251365\n",
      "Train Epoch: 0 [2500/19646 (13%)]\tLoss: 1.239779\n",
      "Train Epoch: 0 [3000/19646 (15%)]\tLoss: 1.305395\n",
      "Train Epoch: 0 [3500/19646 (18%)]\tLoss: 0.867436\n",
      "Train Epoch: 0 [4000/19646 (20%)]\tLoss: 1.337022\n",
      "Train Epoch: 0 [4500/19646 (23%)]\tLoss: 1.313231\n",
      "Train Epoch: 0 [5000/19646 (25%)]\tLoss: 1.432495\n",
      "Train Epoch: 0 [5500/19646 (28%)]\tLoss: 1.342208\n",
      "Train Epoch: 0 [6000/19646 (31%)]\tLoss: 1.420188\n",
      "Train Epoch: 0 [6500/19646 (33%)]\tLoss: 1.058241\n",
      "Train Epoch: 0 [7000/19646 (36%)]\tLoss: 0.883574\n",
      "Train Epoch: 0 [7500/19646 (38%)]\tLoss: 1.256218\n",
      "Train Epoch: 0 [8000/19646 (41%)]\tLoss: 0.783991\n",
      "Train Epoch: 0 [8500/19646 (43%)]\tLoss: 1.107448\n",
      "Train Epoch: 0 [9000/19646 (46%)]\tLoss: 1.127485\n",
      "Train Epoch: 0 [9500/19646 (48%)]\tLoss: 0.573150\n",
      "Train Epoch: 0 [10000/19646 (51%)]\tLoss: 1.252868\n",
      "Train Epoch: 0 [10500/19646 (53%)]\tLoss: 1.153886\n",
      "Train Epoch: 0 [11000/19646 (56%)]\tLoss: 0.771628\n",
      "Train Epoch: 0 [11500/19646 (59%)]\tLoss: 1.029440\n",
      "Train Epoch: 0 [12000/19646 (61%)]\tLoss: 0.674981\n",
      "Train Epoch: 0 [12500/19646 (64%)]\tLoss: 1.228865\n",
      "Train Epoch: 0 [13000/19646 (66%)]\tLoss: 0.816294\n",
      "Train Epoch: 0 [13500/19646 (69%)]\tLoss: 0.941741\n",
      "Train Epoch: 0 [14000/19646 (71%)]\tLoss: 1.326078\n",
      "Train Epoch: 0 [14500/19646 (74%)]\tLoss: 0.699869\n",
      "Train Epoch: 0 [15000/19646 (76%)]\tLoss: 0.401614\n",
      "Train Epoch: 0 [15500/19646 (79%)]\tLoss: 0.484482\n",
      "Train Epoch: 0 [16000/19646 (81%)]\tLoss: 0.624215\n",
      "Train Epoch: 0 [16500/19646 (84%)]\tLoss: 1.000328\n",
      "Train Epoch: 0 [17000/19646 (87%)]\tLoss: 0.975418\n",
      "Train Epoch: 0 [17500/19646 (89%)]\tLoss: 0.265842\n",
      "Train Epoch: 0 [18000/19646 (92%)]\tLoss: 0.643953\n",
      "Train Epoch: 0 [18500/19646 (94%)]\tLoss: 0.367123\n",
      "Train Epoch: 0 [19000/19646 (97%)]\tLoss: 1.572384\n",
      "Train Epoch: 0 [19500/19646 (99%)]\tLoss: 0.482997\n",
      "Training Time: 1516.672904253006   Learning Rate: 0.001\n",
      "0\n",
      "Test Loss is 0.864739, mean precision is: 14.0379%\n",
      "Train Epoch: 1 [0/19646 (0%)]\tLoss: 0.585995\n",
      "Train Epoch: 1 [500/19646 (3%)]\tLoss: 0.667279\n",
      "Train Epoch: 1 [1000/19646 (5%)]\tLoss: 2.126294\n",
      "Train Epoch: 1 [1500/19646 (8%)]\tLoss: 0.655806\n",
      "Train Epoch: 1 [2000/19646 (10%)]\tLoss: 0.847886\n",
      "Train Epoch: 1 [2500/19646 (13%)]\tLoss: 0.597722\n",
      "Train Epoch: 1 [3000/19646 (15%)]\tLoss: 1.123437\n",
      "Train Epoch: 1 [3500/19646 (18%)]\tLoss: 0.428505\n",
      "Train Epoch: 1 [4000/19646 (20%)]\tLoss: 0.439978\n",
      "Train Epoch: 1 [4500/19646 (23%)]\tLoss: 0.439493\n",
      "Train Epoch: 1 [5000/19646 (25%)]\tLoss: 0.813168\n",
      "Train Epoch: 1 [5500/19646 (28%)]\tLoss: 0.690464\n",
      "Train Epoch: 1 [6000/19646 (31%)]\tLoss: 0.971404\n",
      "Train Epoch: 1 [6500/19646 (33%)]\tLoss: 1.312730\n",
      "Train Epoch: 1 [7000/19646 (36%)]\tLoss: 1.168504\n",
      "Train Epoch: 1 [7500/19646 (38%)]\tLoss: 1.034169\n",
      "Train Epoch: 1 [8000/19646 (41%)]\tLoss: 0.587784\n",
      "Train Epoch: 1 [8500/19646 (43%)]\tLoss: 0.845744\n",
      "Train Epoch: 1 [9000/19646 (46%)]\tLoss: 0.639345\n",
      "Train Epoch: 1 [9500/19646 (48%)]\tLoss: 0.483438\n",
      "Train Epoch: 1 [10000/19646 (51%)]\tLoss: 0.522150\n",
      "Train Epoch: 1 [10500/19646 (53%)]\tLoss: 0.586791\n",
      "Train Epoch: 1 [11000/19646 (56%)]\tLoss: 0.867711\n",
      "Train Epoch: 1 [11500/19646 (59%)]\tLoss: 0.417278\n",
      "Train Epoch: 1 [12000/19646 (61%)]\tLoss: 0.838093\n",
      "Train Epoch: 1 [12500/19646 (64%)]\tLoss: 1.234766\n",
      "Train Epoch: 1 [13000/19646 (66%)]\tLoss: 0.846221\n",
      "Train Epoch: 1 [13500/19646 (69%)]\tLoss: 0.554700\n",
      "Train Epoch: 1 [14000/19646 (71%)]\tLoss: 0.836777\n",
      "Train Epoch: 1 [14500/19646 (74%)]\tLoss: 0.907308\n",
      "Train Epoch: 1 [15000/19646 (76%)]\tLoss: 0.329647\n",
      "Train Epoch: 1 [15500/19646 (79%)]\tLoss: 1.050849\n",
      "Train Epoch: 1 [16000/19646 (81%)]\tLoss: 0.818774\n",
      "Train Epoch: 1 [16500/19646 (84%)]\tLoss: 0.420173\n",
      "Train Epoch: 1 [17000/19646 (87%)]\tLoss: 0.525325\n",
      "Train Epoch: 1 [17500/19646 (89%)]\tLoss: 0.647770\n",
      "Train Epoch: 1 [18000/19646 (92%)]\tLoss: 1.050536\n",
      "Train Epoch: 1 [18500/19646 (94%)]\tLoss: 1.734288\n",
      "Train Epoch: 1 [19000/19646 (97%)]\tLoss: 0.966403\n",
      "Train Epoch: 1 [19500/19646 (99%)]\tLoss: 0.739870\n",
      "Training Time: 1522.7208468914032   Learning Rate: 0.001\n",
      "Train Epoch: 2 [0/19646 (0%)]\tLoss: 0.690203\n",
      "Train Epoch: 2 [500/19646 (3%)]\tLoss: 0.739897\n",
      "Train Epoch: 2 [1000/19646 (5%)]\tLoss: 0.835205\n",
      "Train Epoch: 2 [1500/19646 (8%)]\tLoss: 1.421034\n",
      "Train Epoch: 2 [2000/19646 (10%)]\tLoss: 1.037403\n",
      "Train Epoch: 2 [2500/19646 (13%)]\tLoss: 0.988391\n",
      "Train Epoch: 2 [3000/19646 (15%)]\tLoss: 0.885381\n",
      "Train Epoch: 2 [3500/19646 (18%)]\tLoss: 1.038453\n",
      "Train Epoch: 2 [4000/19646 (20%)]\tLoss: 0.989718\n",
      "Train Epoch: 2 [4500/19646 (23%)]\tLoss: 0.843693\n",
      "Train Epoch: 2 [5000/19646 (25%)]\tLoss: 0.275938\n",
      "Train Epoch: 2 [5500/19646 (28%)]\tLoss: 0.433966\n",
      "Train Epoch: 2 [6000/19646 (31%)]\tLoss: 0.745135\n",
      "Train Epoch: 2 [6500/19646 (33%)]\tLoss: 1.353682\n",
      "Train Epoch: 2 [7000/19646 (36%)]\tLoss: 0.652791\n",
      "Train Epoch: 2 [7500/19646 (38%)]\tLoss: 1.493950\n",
      "Train Epoch: 2 [8000/19646 (41%)]\tLoss: 0.832585\n",
      "Train Epoch: 2 [8500/19646 (43%)]\tLoss: 0.826333\n",
      "Train Epoch: 2 [9000/19646 (46%)]\tLoss: 0.264726\n",
      "Train Epoch: 2 [9500/19646 (48%)]\tLoss: 0.761921\n",
      "Train Epoch: 2 [10000/19646 (51%)]\tLoss: 0.708282\n",
      "Train Epoch: 2 [10500/19646 (53%)]\tLoss: 0.569657\n",
      "Train Epoch: 2 [11000/19646 (56%)]\tLoss: 0.886687\n",
      "Train Epoch: 2 [11500/19646 (59%)]\tLoss: 0.652178\n",
      "Train Epoch: 2 [12000/19646 (61%)]\tLoss: 1.055643\n",
      "Train Epoch: 2 [12500/19646 (64%)]\tLoss: 0.506851\n",
      "Train Epoch: 2 [13000/19646 (66%)]\tLoss: 0.916567\n",
      "Train Epoch: 2 [13500/19646 (69%)]\tLoss: 1.030913\n",
      "Train Epoch: 2 [14000/19646 (71%)]\tLoss: 0.650265\n",
      "Train Epoch: 2 [14500/19646 (74%)]\tLoss: 0.752224\n",
      "Train Epoch: 2 [15000/19646 (76%)]\tLoss: 0.547749\n",
      "Train Epoch: 2 [15500/19646 (79%)]\tLoss: 0.786003\n",
      "Train Epoch: 2 [16000/19646 (81%)]\tLoss: 0.919454\n",
      "Train Epoch: 2 [16500/19646 (84%)]\tLoss: 0.418596\n",
      "Train Epoch: 2 [17000/19646 (87%)]\tLoss: 0.865854\n",
      "Train Epoch: 2 [17500/19646 (89%)]\tLoss: 0.209680\n",
      "Train Epoch: 2 [18000/19646 (92%)]\tLoss: 1.026651\n",
      "Train Epoch: 2 [18500/19646 (94%)]\tLoss: 0.739555\n",
      "Train Epoch: 2 [19000/19646 (97%)]\tLoss: 1.207927\n",
      "Train Epoch: 2 [19500/19646 (99%)]\tLoss: 1.330976\n",
      "Training Time: 1525.958727836609   Learning Rate: 0.001\n",
      "2\n",
      "Test Loss is 0.620138, mean precision is: 23.2863%\n",
      "Train Epoch: 3 [0/19646 (0%)]\tLoss: 0.781572\n",
      "Train Epoch: 3 [500/19646 (3%)]\tLoss: 0.623617\n",
      "Train Epoch: 3 [1000/19646 (5%)]\tLoss: 0.821073\n",
      "Train Epoch: 3 [1500/19646 (8%)]\tLoss: 0.984908\n",
      "Train Epoch: 3 [2000/19646 (10%)]\tLoss: 0.769135\n",
      "Train Epoch: 3 [2500/19646 (13%)]\tLoss: 0.923078\n",
      "Train Epoch: 3 [3000/19646 (15%)]\tLoss: 0.682445\n",
      "Train Epoch: 3 [3500/19646 (18%)]\tLoss: 1.169902\n",
      "Train Epoch: 3 [4000/19646 (20%)]\tLoss: 0.890442\n",
      "Train Epoch: 3 [4500/19646 (23%)]\tLoss: 0.751486\n",
      "Train Epoch: 3 [5000/19646 (25%)]\tLoss: 0.476465\n",
      "Train Epoch: 3 [5500/19646 (28%)]\tLoss: 0.752751\n",
      "Train Epoch: 3 [6000/19646 (31%)]\tLoss: 0.809353\n",
      "Train Epoch: 3 [6500/19646 (33%)]\tLoss: 0.686934\n",
      "Train Epoch: 3 [7000/19646 (36%)]\tLoss: 1.801476\n",
      "Train Epoch: 3 [7500/19646 (38%)]\tLoss: 0.496502\n",
      "Train Epoch: 3 [8000/19646 (41%)]\tLoss: 0.912258\n",
      "Train Epoch: 3 [8500/19646 (43%)]\tLoss: 0.583430\n",
      "Train Epoch: 3 [9000/19646 (46%)]\tLoss: 0.886504\n",
      "Train Epoch: 3 [9500/19646 (48%)]\tLoss: 0.440942\n",
      "Train Epoch: 3 [10000/19646 (51%)]\tLoss: 1.198864\n",
      "Train Epoch: 3 [10500/19646 (53%)]\tLoss: 0.568826\n",
      "Train Epoch: 3 [11000/19646 (56%)]\tLoss: 0.605436\n",
      "Train Epoch: 3 [11500/19646 (59%)]\tLoss: 0.221172\n",
      "Train Epoch: 3 [12000/19646 (61%)]\tLoss: 1.385300\n",
      "Train Epoch: 3 [12500/19646 (64%)]\tLoss: 0.527320\n",
      "Train Epoch: 3 [13000/19646 (66%)]\tLoss: 0.885128\n",
      "Train Epoch: 3 [13500/19646 (69%)]\tLoss: 0.761874\n",
      "Train Epoch: 3 [14000/19646 (71%)]\tLoss: 1.267844\n",
      "Train Epoch: 3 [14500/19646 (74%)]\tLoss: 0.939025\n",
      "Train Epoch: 3 [15000/19646 (76%)]\tLoss: 0.601221\n",
      "Train Epoch: 3 [15500/19646 (79%)]\tLoss: 0.471946\n",
      "Train Epoch: 3 [16000/19646 (81%)]\tLoss: 0.767758\n",
      "Train Epoch: 3 [16500/19646 (84%)]\tLoss: 0.452279\n",
      "Train Epoch: 3 [17000/19646 (87%)]\tLoss: 0.861804\n",
      "Train Epoch: 3 [17500/19646 (89%)]\tLoss: 0.600318\n",
      "Train Epoch: 3 [18000/19646 (92%)]\tLoss: 0.695105\n",
      "Train Epoch: 3 [18500/19646 (94%)]\tLoss: 0.848146\n",
      "Train Epoch: 3 [19000/19646 (97%)]\tLoss: 1.332128\n",
      "Train Epoch: 3 [19500/19646 (99%)]\tLoss: 0.827851\n",
      "Training Time: 1524.572214603424   Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/19646 (0%)]\tLoss: 0.968965\n",
      "Train Epoch: 4 [500/19646 (3%)]\tLoss: 0.577200\n",
      "Train Epoch: 4 [1000/19646 (5%)]\tLoss: 0.768917\n",
      "Train Epoch: 4 [1500/19646 (8%)]\tLoss: 0.277463\n",
      "Train Epoch: 4 [2000/19646 (10%)]\tLoss: 0.494759\n",
      "Train Epoch: 4 [2500/19646 (13%)]\tLoss: 1.326341\n",
      "Train Epoch: 4 [3000/19646 (15%)]\tLoss: 0.593986\n",
      "Train Epoch: 4 [3500/19646 (18%)]\tLoss: 0.358630\n",
      "Train Epoch: 4 [4000/19646 (20%)]\tLoss: 0.688990\n",
      "Train Epoch: 4 [4500/19646 (23%)]\tLoss: 0.751929\n",
      "Train Epoch: 4 [5000/19646 (25%)]\tLoss: 0.717903\n",
      "Train Epoch: 4 [5500/19646 (28%)]\tLoss: 0.899757\n",
      "Train Epoch: 4 [6000/19646 (31%)]\tLoss: 0.453368\n",
      "Train Epoch: 4 [6500/19646 (33%)]\tLoss: 1.070116\n",
      "Train Epoch: 4 [7000/19646 (36%)]\tLoss: 0.693690\n",
      "Train Epoch: 4 [7500/19646 (38%)]\tLoss: 1.960606\n",
      "Train Epoch: 4 [8000/19646 (41%)]\tLoss: 0.574506\n",
      "Train Epoch: 4 [8500/19646 (43%)]\tLoss: 1.379086\n",
      "Train Epoch: 4 [9000/19646 (46%)]\tLoss: 0.586502\n",
      "Train Epoch: 4 [9500/19646 (48%)]\tLoss: 0.555719\n",
      "Train Epoch: 4 [10000/19646 (51%)]\tLoss: 1.157345\n",
      "Train Epoch: 4 [10500/19646 (53%)]\tLoss: 0.428891\n",
      "Train Epoch: 4 [11000/19646 (56%)]\tLoss: 0.830953\n",
      "Train Epoch: 4 [11500/19646 (59%)]\tLoss: 0.841659\n",
      "Train Epoch: 4 [12000/19646 (61%)]\tLoss: 0.672060\n",
      "Train Epoch: 4 [12500/19646 (64%)]\tLoss: 0.264371\n",
      "Train Epoch: 4 [13000/19646 (66%)]\tLoss: 1.005573\n",
      "Train Epoch: 4 [13500/19646 (69%)]\tLoss: 0.223661\n",
      "Train Epoch: 4 [14000/19646 (71%)]\tLoss: 0.296521\n",
      "Train Epoch: 4 [14500/19646 (74%)]\tLoss: 1.182810\n",
      "Train Epoch: 4 [15000/19646 (76%)]\tLoss: 0.419472\n",
      "Train Epoch: 4 [15500/19646 (79%)]\tLoss: 0.832725\n",
      "Train Epoch: 4 [16000/19646 (81%)]\tLoss: 0.749917\n",
      "Train Epoch: 4 [16500/19646 (84%)]\tLoss: 0.573866\n",
      "Train Epoch: 4 [17000/19646 (87%)]\tLoss: 0.980581\n",
      "Train Epoch: 4 [17500/19646 (89%)]\tLoss: 0.672246\n",
      "Train Epoch: 4 [18000/19646 (92%)]\tLoss: 0.522449\n",
      "Train Epoch: 4 [18500/19646 (94%)]\tLoss: 0.332670\n",
      "Train Epoch: 4 [19000/19646 (97%)]\tLoss: 0.942943\n",
      "Train Epoch: 4 [19500/19646 (99%)]\tLoss: 0.336168\n",
      "Training Time: 1526.9287221431732   Learning Rate: 0.001\n",
      "4\n",
      "Test Loss is 0.608575, mean precision is: 26.9152%\n",
      "Train Epoch: 5 [0/19646 (0%)]\tLoss: 0.383312\n",
      "Train Epoch: 5 [500/19646 (3%)]\tLoss: 0.697512\n",
      "Train Epoch: 5 [1000/19646 (5%)]\tLoss: 0.939662\n",
      "Train Epoch: 5 [1500/19646 (8%)]\tLoss: 0.682054\n",
      "Train Epoch: 5 [2000/19646 (10%)]\tLoss: 0.940072\n",
      "Train Epoch: 5 [2500/19646 (13%)]\tLoss: 0.655909\n",
      "Train Epoch: 5 [3000/19646 (15%)]\tLoss: 0.297067\n",
      "Train Epoch: 5 [3500/19646 (18%)]\tLoss: 1.832006\n",
      "Train Epoch: 5 [4000/19646 (20%)]\tLoss: 0.713356\n",
      "Train Epoch: 5 [4500/19646 (23%)]\tLoss: 0.553002\n",
      "Train Epoch: 5 [5000/19646 (25%)]\tLoss: 0.847607\n",
      "Train Epoch: 5 [5500/19646 (28%)]\tLoss: 0.744769\n",
      "Train Epoch: 5 [6000/19646 (31%)]\tLoss: 0.380657\n",
      "Train Epoch: 5 [6500/19646 (33%)]\tLoss: 0.720347\n",
      "Train Epoch: 5 [7000/19646 (36%)]\tLoss: 0.332390\n",
      "Train Epoch: 5 [7500/19646 (38%)]\tLoss: 0.741842\n",
      "Train Epoch: 5 [8000/19646 (41%)]\tLoss: 0.676882\n",
      "Train Epoch: 5 [8500/19646 (43%)]\tLoss: 0.670769\n",
      "Train Epoch: 5 [9000/19646 (46%)]\tLoss: 0.692759\n",
      "Train Epoch: 5 [9500/19646 (48%)]\tLoss: 0.476929\n",
      "Train Epoch: 5 [10000/19646 (51%)]\tLoss: 0.631328\n",
      "Train Epoch: 5 [10500/19646 (53%)]\tLoss: 0.531866\n",
      "Train Epoch: 5 [11000/19646 (56%)]\tLoss: 0.827169\n",
      "Train Epoch: 5 [11500/19646 (59%)]\tLoss: 0.271796\n",
      "Train Epoch: 5 [12000/19646 (61%)]\tLoss: 1.291203\n",
      "Train Epoch: 5 [12500/19646 (64%)]\tLoss: 0.595047\n",
      "Train Epoch: 5 [13000/19646 (66%)]\tLoss: 0.564317\n",
      "Train Epoch: 5 [13500/19646 (69%)]\tLoss: 0.648170\n",
      "Train Epoch: 5 [14000/19646 (71%)]\tLoss: 0.515275\n",
      "Train Epoch: 5 [14500/19646 (74%)]\tLoss: 0.411421\n",
      "Train Epoch: 5 [15000/19646 (76%)]\tLoss: 0.900609\n",
      "Train Epoch: 5 [15500/19646 (79%)]\tLoss: 0.649756\n",
      "Train Epoch: 5 [16000/19646 (81%)]\tLoss: 0.447896\n",
      "Train Epoch: 5 [16500/19646 (84%)]\tLoss: 0.211397\n",
      "Train Epoch: 5 [17000/19646 (87%)]\tLoss: 0.523215\n",
      "Train Epoch: 5 [17500/19646 (89%)]\tLoss: 0.559422\n",
      "Train Epoch: 5 [18000/19646 (92%)]\tLoss: 0.329527\n",
      "Train Epoch: 5 [18500/19646 (94%)]\tLoss: 0.639726\n",
      "Train Epoch: 5 [19000/19646 (97%)]\tLoss: 0.446851\n",
      "Train Epoch: 5 [19500/19646 (99%)]\tLoss: 0.846554\n",
      "Training Time: 1525.8644642829895   Learning Rate: 0.001\n",
      "Train Epoch: 6 [0/19646 (0%)]\tLoss: 0.776043\n",
      "Train Epoch: 6 [500/19646 (3%)]\tLoss: 0.767805\n",
      "Train Epoch: 6 [1000/19646 (5%)]\tLoss: 0.823705\n",
      "Train Epoch: 6 [1500/19646 (8%)]\tLoss: 0.264714\n",
      "Train Epoch: 6 [2000/19646 (10%)]\tLoss: 0.359576\n",
      "Train Epoch: 6 [2500/19646 (13%)]\tLoss: 0.354027\n",
      "Train Epoch: 6 [3000/19646 (15%)]\tLoss: 0.421042\n",
      "Train Epoch: 6 [3500/19646 (18%)]\tLoss: 0.444490\n",
      "Train Epoch: 6 [4000/19646 (20%)]\tLoss: 0.768940\n",
      "Train Epoch: 6 [4500/19646 (23%)]\tLoss: 0.835434\n",
      "Train Epoch: 6 [5000/19646 (25%)]\tLoss: 0.430379\n",
      "Train Epoch: 6 [5500/19646 (28%)]\tLoss: 1.455747\n",
      "Train Epoch: 6 [6000/19646 (31%)]\tLoss: 0.565898\n",
      "Train Epoch: 6 [6500/19646 (33%)]\tLoss: 0.365044\n",
      "Train Epoch: 6 [7000/19646 (36%)]\tLoss: 0.757687\n",
      "Train Epoch: 6 [7500/19646 (38%)]\tLoss: 1.026173\n",
      "Train Epoch: 6 [8000/19646 (41%)]\tLoss: 1.213854\n",
      "Train Epoch: 6 [8500/19646 (43%)]\tLoss: 0.558338\n",
      "Train Epoch: 6 [9000/19646 (46%)]\tLoss: 0.928593\n",
      "Train Epoch: 6 [9500/19646 (48%)]\tLoss: 0.518278\n",
      "Train Epoch: 6 [10000/19646 (51%)]\tLoss: 0.810024\n",
      "Train Epoch: 6 [10500/19646 (53%)]\tLoss: 0.753936\n",
      "Train Epoch: 6 [11000/19646 (56%)]\tLoss: 0.532747\n",
      "Train Epoch: 6 [11500/19646 (59%)]\tLoss: 0.854758\n",
      "Train Epoch: 6 [12000/19646 (61%)]\tLoss: 0.626082\n",
      "Train Epoch: 6 [12500/19646 (64%)]\tLoss: 0.475028\n",
      "Train Epoch: 6 [13000/19646 (66%)]\tLoss: 0.609010\n",
      "Train Epoch: 6 [13500/19646 (69%)]\tLoss: 0.525700\n",
      "Train Epoch: 6 [14000/19646 (71%)]\tLoss: 0.538424\n",
      "Train Epoch: 6 [14500/19646 (74%)]\tLoss: 0.730573\n",
      "Train Epoch: 6 [15000/19646 (76%)]\tLoss: 0.363750\n",
      "Train Epoch: 6 [15500/19646 (79%)]\tLoss: 0.412249\n",
      "Train Epoch: 6 [16000/19646 (81%)]\tLoss: 0.607480\n",
      "Train Epoch: 6 [16500/19646 (84%)]\tLoss: 0.648915\n",
      "Train Epoch: 6 [17000/19646 (87%)]\tLoss: 1.036731\n",
      "Train Epoch: 6 [17500/19646 (89%)]\tLoss: 0.318010\n",
      "Train Epoch: 6 [18000/19646 (92%)]\tLoss: 0.406124\n",
      "Train Epoch: 6 [18500/19646 (94%)]\tLoss: 0.506687\n",
      "Train Epoch: 6 [19000/19646 (97%)]\tLoss: 0.492493\n",
      "Train Epoch: 6 [19500/19646 (99%)]\tLoss: 0.260810\n",
      "Training Time: 1528.8506762981415   Learning Rate: 0.001\n",
      "6\n",
      "Test Loss is 0.605966, mean precision is: 22.9709%\n",
      "Train Epoch: 7 [0/19646 (0%)]\tLoss: 0.628897\n",
      "Train Epoch: 7 [500/19646 (3%)]\tLoss: 0.689508\n",
      "Train Epoch: 7 [1000/19646 (5%)]\tLoss: 0.209159\n",
      "Train Epoch: 7 [1500/19646 (8%)]\tLoss: 0.502642\n",
      "Train Epoch: 7 [2000/19646 (10%)]\tLoss: 0.587034\n",
      "Train Epoch: 7 [2500/19646 (13%)]\tLoss: 0.756166\n",
      "Train Epoch: 7 [3000/19646 (15%)]\tLoss: 0.404599\n",
      "Train Epoch: 7 [3500/19646 (18%)]\tLoss: 0.520725\n",
      "Train Epoch: 7 [4000/19646 (20%)]\tLoss: 0.295042\n",
      "Train Epoch: 7 [4500/19646 (23%)]\tLoss: 2.064833\n",
      "Train Epoch: 7 [5000/19646 (25%)]\tLoss: 1.380242\n",
      "Train Epoch: 7 [5500/19646 (28%)]\tLoss: 0.910795\n",
      "Train Epoch: 7 [6000/19646 (31%)]\tLoss: 0.577087\n",
      "Train Epoch: 7 [6500/19646 (33%)]\tLoss: 0.753162\n",
      "Train Epoch: 7 [7000/19646 (36%)]\tLoss: 0.233249\n",
      "Train Epoch: 7 [7500/19646 (38%)]\tLoss: 0.263943\n",
      "Train Epoch: 7 [8000/19646 (41%)]\tLoss: 1.218788\n",
      "Train Epoch: 7 [8500/19646 (43%)]\tLoss: 0.563860\n",
      "Train Epoch: 7 [9000/19646 (46%)]\tLoss: 0.998104\n",
      "Train Epoch: 7 [9500/19646 (48%)]\tLoss: 0.458281\n",
      "Train Epoch: 7 [10000/19646 (51%)]\tLoss: 0.710908\n",
      "Train Epoch: 7 [10500/19646 (53%)]\tLoss: 0.472055\n",
      "Train Epoch: 7 [11000/19646 (56%)]\tLoss: 0.802346\n",
      "Train Epoch: 7 [11500/19646 (59%)]\tLoss: 0.384981\n",
      "Train Epoch: 7 [12000/19646 (61%)]\tLoss: 0.638231\n",
      "Train Epoch: 7 [12500/19646 (64%)]\tLoss: 0.435171\n",
      "Train Epoch: 7 [13000/19646 (66%)]\tLoss: 0.528930\n",
      "Train Epoch: 7 [13500/19646 (69%)]\tLoss: 0.222444\n",
      "Train Epoch: 7 [14000/19646 (71%)]\tLoss: 0.306537\n",
      "Train Epoch: 7 [14500/19646 (74%)]\tLoss: 0.317496\n",
      "Train Epoch: 7 [15000/19646 (76%)]\tLoss: 0.202859\n",
      "Train Epoch: 7 [15500/19646 (79%)]\tLoss: 0.144210\n",
      "Train Epoch: 7 [16000/19646 (81%)]\tLoss: 0.569651\n",
      "Train Epoch: 7 [16500/19646 (84%)]\tLoss: 0.880938\n",
      "Train Epoch: 7 [17000/19646 (87%)]\tLoss: 0.722205\n",
      "Train Epoch: 7 [17500/19646 (89%)]\tLoss: 0.612882\n",
      "Train Epoch: 7 [18000/19646 (92%)]\tLoss: 0.406215\n",
      "Train Epoch: 7 [18500/19646 (94%)]\tLoss: 0.227602\n",
      "Train Epoch: 7 [19000/19646 (97%)]\tLoss: 0.840805\n",
      "Train Epoch: 7 [19500/19646 (99%)]\tLoss: 0.257080\n",
      "Training Time: 1522.240835428238   Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/19646 (0%)]\tLoss: 0.195350\n",
      "Train Epoch: 8 [500/19646 (3%)]\tLoss: 1.080519\n",
      "Train Epoch: 8 [1000/19646 (5%)]\tLoss: 0.652250\n",
      "Train Epoch: 8 [1500/19646 (8%)]\tLoss: 0.253892\n",
      "Train Epoch: 8 [2000/19646 (10%)]\tLoss: 0.837944\n",
      "Train Epoch: 8 [2500/19646 (13%)]\tLoss: 0.569684\n",
      "Train Epoch: 8 [3000/19646 (15%)]\tLoss: 0.367797\n",
      "Train Epoch: 8 [3500/19646 (18%)]\tLoss: 0.496820\n",
      "Train Epoch: 8 [4000/19646 (20%)]\tLoss: 0.163337\n",
      "Train Epoch: 8 [4500/19646 (23%)]\tLoss: 0.194475\n",
      "Train Epoch: 8 [5000/19646 (25%)]\tLoss: 0.554177\n",
      "Train Epoch: 8 [5500/19646 (28%)]\tLoss: 0.913686\n",
      "Train Epoch: 8 [6000/19646 (31%)]\tLoss: 0.659710\n",
      "Train Epoch: 8 [6500/19646 (33%)]\tLoss: 0.630284\n",
      "Train Epoch: 8 [7000/19646 (36%)]\tLoss: 0.638162\n",
      "Train Epoch: 8 [7500/19646 (38%)]\tLoss: 0.593452\n",
      "Train Epoch: 8 [8000/19646 (41%)]\tLoss: 0.504524\n",
      "Train Epoch: 8 [8500/19646 (43%)]\tLoss: 0.877739\n",
      "Train Epoch: 8 [9000/19646 (46%)]\tLoss: 0.839584\n",
      "Train Epoch: 8 [9500/19646 (48%)]\tLoss: 0.776198\n",
      "Train Epoch: 8 [10000/19646 (51%)]\tLoss: 0.534960\n",
      "Train Epoch: 8 [10500/19646 (53%)]\tLoss: 0.199775\n",
      "Train Epoch: 8 [11000/19646 (56%)]\tLoss: 0.686712\n",
      "Train Epoch: 8 [11500/19646 (59%)]\tLoss: 0.671973\n",
      "Train Epoch: 8 [12000/19646 (61%)]\tLoss: 0.585447\n",
      "Train Epoch: 8 [12500/19646 (64%)]\tLoss: 0.569440\n",
      "Train Epoch: 8 [13000/19646 (66%)]\tLoss: 0.951562\n",
      "Train Epoch: 8 [13500/19646 (69%)]\tLoss: 0.450159\n",
      "Train Epoch: 8 [14000/19646 (71%)]\tLoss: 0.474251\n",
      "Train Epoch: 8 [14500/19646 (74%)]\tLoss: 0.400604\n",
      "Train Epoch: 8 [15000/19646 (76%)]\tLoss: 0.498306\n",
      "Train Epoch: 8 [15500/19646 (79%)]\tLoss: 0.337564\n",
      "Train Epoch: 8 [16000/19646 (81%)]\tLoss: 0.944525\n",
      "Train Epoch: 8 [16500/19646 (84%)]\tLoss: 0.356476\n",
      "Train Epoch: 8 [17000/19646 (87%)]\tLoss: 0.238536\n",
      "Train Epoch: 8 [17500/19646 (89%)]\tLoss: 0.449416\n",
      "Train Epoch: 8 [18000/19646 (92%)]\tLoss: 0.630256\n",
      "Train Epoch: 8 [18500/19646 (94%)]\tLoss: 0.329618\n",
      "Train Epoch: 8 [19000/19646 (97%)]\tLoss: 0.400543\n",
      "Train Epoch: 8 [19500/19646 (99%)]\tLoss: 0.155117\n",
      "Training Time: 1523.8585550785065   Learning Rate: 0.001\n",
      "8\n",
      "Test Loss is 0.565870, mean precision is: 28.0222%\n",
      "Train Epoch: 9 [0/19646 (0%)]\tLoss: 0.639700\n",
      "Train Epoch: 9 [500/19646 (3%)]\tLoss: 0.775649\n",
      "Train Epoch: 9 [1000/19646 (5%)]\tLoss: 0.432746\n",
      "Train Epoch: 9 [1500/19646 (8%)]\tLoss: 0.925904\n",
      "Train Epoch: 9 [2000/19646 (10%)]\tLoss: 0.695719\n",
      "Train Epoch: 9 [2500/19646 (13%)]\tLoss: 0.606701\n",
      "Train Epoch: 9 [3000/19646 (15%)]\tLoss: 0.846322\n",
      "Train Epoch: 9 [3500/19646 (18%)]\tLoss: 1.225175\n",
      "Train Epoch: 9 [4000/19646 (20%)]\tLoss: 0.339582\n",
      "Train Epoch: 9 [4500/19646 (23%)]\tLoss: 0.323234\n",
      "Train Epoch: 9 [5000/19646 (25%)]\tLoss: 0.687008\n",
      "Train Epoch: 9 [5500/19646 (28%)]\tLoss: 0.220558\n",
      "Train Epoch: 9 [6000/19646 (31%)]\tLoss: 0.583017\n",
      "Train Epoch: 9 [6500/19646 (33%)]\tLoss: 0.608441\n",
      "Train Epoch: 9 [7000/19646 (36%)]\tLoss: 0.419834\n",
      "Train Epoch: 9 [7500/19646 (38%)]\tLoss: 0.269431\n",
      "Train Epoch: 9 [8000/19646 (41%)]\tLoss: 0.554025\n",
      "Train Epoch: 9 [8500/19646 (43%)]\tLoss: 0.620733\n",
      "Train Epoch: 9 [9000/19646 (46%)]\tLoss: 1.000487\n",
      "Train Epoch: 9 [9500/19646 (48%)]\tLoss: 0.222166\n",
      "Train Epoch: 9 [10000/19646 (51%)]\tLoss: 0.493833\n",
      "Train Epoch: 9 [10500/19646 (53%)]\tLoss: 2.276709\n",
      "Train Epoch: 9 [11000/19646 (56%)]\tLoss: 0.253437\n",
      "Train Epoch: 9 [11500/19646 (59%)]\tLoss: 0.892395\n",
      "Train Epoch: 9 [12000/19646 (61%)]\tLoss: 0.203695\n",
      "Train Epoch: 9 [12500/19646 (64%)]\tLoss: 0.585074\n",
      "Train Epoch: 9 [13000/19646 (66%)]\tLoss: 0.307844\n",
      "Train Epoch: 9 [13500/19646 (69%)]\tLoss: 0.476958\n",
      "Train Epoch: 9 [14000/19646 (71%)]\tLoss: 0.737066\n",
      "Train Epoch: 9 [14500/19646 (74%)]\tLoss: 0.832507\n",
      "Train Epoch: 9 [15000/19646 (76%)]\tLoss: 0.590466\n",
      "Train Epoch: 9 [15500/19646 (79%)]\tLoss: 0.749695\n",
      "Train Epoch: 9 [16000/19646 (81%)]\tLoss: 0.468381\n",
      "Train Epoch: 9 [16500/19646 (84%)]\tLoss: 0.661617\n",
      "Train Epoch: 9 [17000/19646 (87%)]\tLoss: 0.723351\n",
      "Train Epoch: 9 [17500/19646 (89%)]\tLoss: 0.580728\n",
      "Train Epoch: 9 [18000/19646 (92%)]\tLoss: 0.910466\n",
      "Train Epoch: 9 [18500/19646 (94%)]\tLoss: 0.739079\n",
      "Train Epoch: 9 [19000/19646 (97%)]\tLoss: 0.477567\n",
      "Train Epoch: 9 [19500/19646 (99%)]\tLoss: 0.257100\n",
      "Training Time: 1525.0593979358673   Learning Rate: 0.001\n",
      "Train Epoch: 10 [0/19646 (0%)]\tLoss: 0.529061\n",
      "Train Epoch: 10 [500/19646 (3%)]\tLoss: 0.499663\n",
      "Train Epoch: 10 [1000/19646 (5%)]\tLoss: 0.449672\n",
      "Train Epoch: 10 [1500/19646 (8%)]\tLoss: 0.391848\n",
      "Train Epoch: 10 [2000/19646 (10%)]\tLoss: 0.925897\n",
      "Train Epoch: 10 [2500/19646 (13%)]\tLoss: 0.438224\n",
      "Train Epoch: 10 [3000/19646 (15%)]\tLoss: 0.351845\n",
      "Train Epoch: 10 [3500/19646 (18%)]\tLoss: 0.387555\n",
      "Train Epoch: 10 [4000/19646 (20%)]\tLoss: 1.255575\n",
      "Train Epoch: 10 [4500/19646 (23%)]\tLoss: 0.367485\n",
      "Train Epoch: 10 [5000/19646 (25%)]\tLoss: 0.573347\n",
      "Train Epoch: 10 [5500/19646 (28%)]\tLoss: 0.379910\n",
      "Train Epoch: 10 [6000/19646 (31%)]\tLoss: 0.429704\n",
      "Train Epoch: 10 [6500/19646 (33%)]\tLoss: 0.267701\n",
      "Train Epoch: 10 [7000/19646 (36%)]\tLoss: 0.379559\n",
      "Train Epoch: 10 [7500/19646 (38%)]\tLoss: 0.276666\n",
      "Train Epoch: 10 [8000/19646 (41%)]\tLoss: 0.230280\n",
      "Train Epoch: 10 [8500/19646 (43%)]\tLoss: 0.417158\n",
      "Train Epoch: 10 [9000/19646 (46%)]\tLoss: 0.464114\n",
      "Train Epoch: 10 [9500/19646 (48%)]\tLoss: 0.499634\n",
      "Train Epoch: 10 [10000/19646 (51%)]\tLoss: 0.379857\n",
      "Train Epoch: 10 [10500/19646 (53%)]\tLoss: 0.724087\n",
      "Train Epoch: 10 [11000/19646 (56%)]\tLoss: 0.276621\n",
      "Train Epoch: 10 [11500/19646 (59%)]\tLoss: 0.550489\n",
      "Train Epoch: 10 [12000/19646 (61%)]\tLoss: 0.411128\n",
      "Train Epoch: 10 [12500/19646 (64%)]\tLoss: 0.807347\n",
      "Train Epoch: 10 [13000/19646 (66%)]\tLoss: 0.367865\n",
      "Train Epoch: 10 [13500/19646 (69%)]\tLoss: 0.438723\n",
      "Train Epoch: 10 [14000/19646 (71%)]\tLoss: 0.510844\n",
      "Train Epoch: 10 [14500/19646 (74%)]\tLoss: 0.644163\n",
      "Train Epoch: 10 [15000/19646 (76%)]\tLoss: 0.783908\n",
      "Train Epoch: 10 [15500/19646 (79%)]\tLoss: 0.933627\n",
      "Train Epoch: 10 [16000/19646 (81%)]\tLoss: 0.447295\n",
      "Train Epoch: 10 [16500/19646 (84%)]\tLoss: 0.403690\n",
      "Train Epoch: 10 [17000/19646 (87%)]\tLoss: 0.580516\n",
      "Train Epoch: 10 [17500/19646 (89%)]\tLoss: 0.658573\n",
      "Train Epoch: 10 [18000/19646 (92%)]\tLoss: 0.353850\n",
      "Train Epoch: 10 [18500/19646 (94%)]\tLoss: 0.427612\n",
      "Train Epoch: 10 [19000/19646 (97%)]\tLoss: 0.356153\n",
      "Train Epoch: 10 [19500/19646 (99%)]\tLoss: 0.837981\n",
      "Training Time: 1526.8990213871002   Learning Rate: 0.0001\n",
      "10\n",
      "Test Loss is 0.498930, mean precision is: 31.1165%\n",
      "Train Epoch: 11 [0/19646 (0%)]\tLoss: 0.601571\n",
      "Train Epoch: 11 [500/19646 (3%)]\tLoss: 1.298758\n",
      "Train Epoch: 11 [1000/19646 (5%)]\tLoss: 1.113213\n",
      "Train Epoch: 11 [1500/19646 (8%)]\tLoss: 0.452957\n",
      "Train Epoch: 11 [2000/19646 (10%)]\tLoss: 0.330999\n",
      "Train Epoch: 11 [2500/19646 (13%)]\tLoss: 0.538284\n",
      "Train Epoch: 11 [3000/19646 (15%)]\tLoss: 0.360560\n",
      "Train Epoch: 11 [3500/19646 (18%)]\tLoss: 0.295659\n",
      "Train Epoch: 11 [4000/19646 (20%)]\tLoss: 0.165049\n",
      "Train Epoch: 11 [4500/19646 (23%)]\tLoss: 0.594838\n",
      "Train Epoch: 11 [5000/19646 (25%)]\tLoss: 0.434999\n",
      "Train Epoch: 11 [5500/19646 (28%)]\tLoss: 0.533634\n",
      "Train Epoch: 11 [6000/19646 (31%)]\tLoss: 0.987009\n",
      "Train Epoch: 11 [6500/19646 (33%)]\tLoss: 0.426591\n",
      "Train Epoch: 11 [7000/19646 (36%)]\tLoss: 0.183181\n",
      "Train Epoch: 11 [7500/19646 (38%)]\tLoss: 0.236485\n",
      "Train Epoch: 11 [8000/19646 (41%)]\tLoss: 0.338850\n",
      "Train Epoch: 11 [8500/19646 (43%)]\tLoss: 0.643386\n",
      "Train Epoch: 11 [9000/19646 (46%)]\tLoss: 0.483038\n",
      "Train Epoch: 11 [9500/19646 (48%)]\tLoss: 0.242174\n",
      "Train Epoch: 11 [10000/19646 (51%)]\tLoss: 0.439343\n",
      "Train Epoch: 11 [10500/19646 (53%)]\tLoss: 0.336997\n",
      "Train Epoch: 11 [11000/19646 (56%)]\tLoss: 0.606635\n",
      "Train Epoch: 11 [11500/19646 (59%)]\tLoss: 0.721646\n",
      "Train Epoch: 11 [12000/19646 (61%)]\tLoss: 0.680130\n",
      "Train Epoch: 11 [12500/19646 (64%)]\tLoss: 0.428708\n",
      "Train Epoch: 11 [13000/19646 (66%)]\tLoss: 0.305889\n",
      "Train Epoch: 11 [13500/19646 (69%)]\tLoss: 0.172028\n",
      "Train Epoch: 11 [14000/19646 (71%)]\tLoss: 0.693615\n",
      "Train Epoch: 11 [14500/19646 (74%)]\tLoss: 0.549794\n",
      "Train Epoch: 11 [15000/19646 (76%)]\tLoss: 0.627485\n",
      "Train Epoch: 11 [15500/19646 (79%)]\tLoss: 0.667730\n",
      "Train Epoch: 11 [16000/19646 (81%)]\tLoss: 0.251654\n",
      "Train Epoch: 11 [16500/19646 (84%)]\tLoss: 0.343437\n",
      "Train Epoch: 11 [17000/19646 (87%)]\tLoss: 0.267287\n",
      "Train Epoch: 11 [17500/19646 (89%)]\tLoss: 0.421699\n",
      "Train Epoch: 11 [18000/19646 (92%)]\tLoss: 0.301433\n",
      "Train Epoch: 11 [18500/19646 (94%)]\tLoss: 1.351160\n",
      "Train Epoch: 11 [19000/19646 (97%)]\tLoss: 0.545423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [19500/19646 (99%)]\tLoss: 0.183310\n",
      "Training Time: 1519.815928220749   Learning Rate: 0.0001\n",
      "Train Epoch: 12 [0/19646 (0%)]\tLoss: 1.001560\n",
      "Train Epoch: 12 [500/19646 (3%)]\tLoss: 0.397972\n",
      "Train Epoch: 12 [1000/19646 (5%)]\tLoss: 0.691446\n",
      "Train Epoch: 12 [1500/19646 (8%)]\tLoss: 0.197493\n",
      "Train Epoch: 12 [2000/19646 (10%)]\tLoss: 0.327336\n",
      "Train Epoch: 12 [2500/19646 (13%)]\tLoss: 0.502368\n",
      "Train Epoch: 12 [3000/19646 (15%)]\tLoss: 0.482265\n",
      "Train Epoch: 12 [3500/19646 (18%)]\tLoss: 0.861272\n",
      "Train Epoch: 12 [4000/19646 (20%)]\tLoss: 2.066965\n",
      "Train Epoch: 12 [4500/19646 (23%)]\tLoss: 0.480590\n",
      "Train Epoch: 12 [5000/19646 (25%)]\tLoss: 1.027512\n",
      "Train Epoch: 12 [5500/19646 (28%)]\tLoss: 0.913225\n",
      "Train Epoch: 12 [6000/19646 (31%)]\tLoss: 0.217693\n",
      "Train Epoch: 12 [6500/19646 (33%)]\tLoss: 0.298114\n",
      "Train Epoch: 12 [7000/19646 (36%)]\tLoss: 0.335706\n",
      "Train Epoch: 12 [7500/19646 (38%)]\tLoss: 0.552958\n",
      "Train Epoch: 12 [8000/19646 (41%)]\tLoss: 0.883675\n",
      "Train Epoch: 12 [8500/19646 (43%)]\tLoss: 0.424364\n",
      "Train Epoch: 12 [9000/19646 (46%)]\tLoss: 0.631336\n",
      "Train Epoch: 12 [9500/19646 (48%)]\tLoss: 0.282121\n",
      "Train Epoch: 12 [10000/19646 (51%)]\tLoss: 0.439215\n",
      "Train Epoch: 12 [10500/19646 (53%)]\tLoss: 0.628756\n",
      "Train Epoch: 12 [11000/19646 (56%)]\tLoss: 0.677257\n",
      "Train Epoch: 12 [11500/19646 (59%)]\tLoss: 0.341294\n",
      "Train Epoch: 12 [12000/19646 (61%)]\tLoss: 0.555430\n",
      "Train Epoch: 12 [12500/19646 (64%)]\tLoss: 0.455126\n",
      "Train Epoch: 12 [13000/19646 (66%)]\tLoss: 1.094230\n",
      "Train Epoch: 12 [13500/19646 (69%)]\tLoss: 0.410246\n",
      "Train Epoch: 12 [14000/19646 (71%)]\tLoss: 0.463588\n",
      "Train Epoch: 12 [14500/19646 (74%)]\tLoss: 0.319697\n",
      "Train Epoch: 12 [15000/19646 (76%)]\tLoss: 0.819310\n",
      "Train Epoch: 12 [15500/19646 (79%)]\tLoss: 0.467730\n",
      "Train Epoch: 12 [16000/19646 (81%)]\tLoss: 0.497876\n",
      "Train Epoch: 12 [16500/19646 (84%)]\tLoss: 0.872284\n",
      "Train Epoch: 12 [17000/19646 (87%)]\tLoss: 0.234638\n",
      "Train Epoch: 12 [17500/19646 (89%)]\tLoss: 0.767214\n",
      "Train Epoch: 12 [18000/19646 (92%)]\tLoss: 0.335046\n",
      "Train Epoch: 12 [18500/19646 (94%)]\tLoss: 0.182167\n",
      "Train Epoch: 12 [19000/19646 (97%)]\tLoss: 0.342142\n",
      "Train Epoch: 12 [19500/19646 (99%)]\tLoss: 0.504945\n",
      "Training Time: 1524.6429884433746   Learning Rate: 0.0001\n",
      "12\n",
      "Test Loss is 0.540621, mean precision is: 28.9639%\n",
      "Train Epoch: 13 [0/19646 (0%)]\tLoss: 0.330264\n",
      "Train Epoch: 13 [500/19646 (3%)]\tLoss: 0.265933\n",
      "Train Epoch: 13 [1000/19646 (5%)]\tLoss: 1.311541\n",
      "Train Epoch: 13 [1500/19646 (8%)]\tLoss: 0.180946\n",
      "Train Epoch: 13 [2000/19646 (10%)]\tLoss: 0.302009\n",
      "Train Epoch: 13 [2500/19646 (13%)]\tLoss: 1.284055\n",
      "Train Epoch: 13 [3000/19646 (15%)]\tLoss: 0.210735\n",
      "Train Epoch: 13 [3500/19646 (18%)]\tLoss: 0.352296\n",
      "Train Epoch: 13 [4000/19646 (20%)]\tLoss: 0.206444\n",
      "Train Epoch: 13 [4500/19646 (23%)]\tLoss: 0.273014\n",
      "Train Epoch: 13 [5000/19646 (25%)]\tLoss: 0.541653\n",
      "Train Epoch: 13 [5500/19646 (28%)]\tLoss: 0.614637\n",
      "Train Epoch: 13 [6000/19646 (31%)]\tLoss: 0.349747\n",
      "Train Epoch: 13 [6500/19646 (33%)]\tLoss: 0.470711\n",
      "Train Epoch: 13 [7000/19646 (36%)]\tLoss: 0.219630\n",
      "Train Epoch: 13 [7500/19646 (38%)]\tLoss: 0.349482\n",
      "Train Epoch: 13 [8000/19646 (41%)]\tLoss: 0.446004\n",
      "Train Epoch: 13 [8500/19646 (43%)]\tLoss: 0.625651\n",
      "Train Epoch: 13 [9000/19646 (46%)]\tLoss: 0.377899\n",
      "Train Epoch: 13 [9500/19646 (48%)]\tLoss: 0.580500\n",
      "Train Epoch: 13 [10000/19646 (51%)]\tLoss: 0.740233\n",
      "Train Epoch: 13 [10500/19646 (53%)]\tLoss: 0.358142\n",
      "Train Epoch: 13 [11000/19646 (56%)]\tLoss: 0.319812\n",
      "Train Epoch: 13 [11500/19646 (59%)]\tLoss: 0.699441\n",
      "Train Epoch: 13 [12000/19646 (61%)]\tLoss: 0.533949\n",
      "Train Epoch: 13 [12500/19646 (64%)]\tLoss: 0.321508\n",
      "Train Epoch: 13 [13000/19646 (66%)]\tLoss: 0.421342\n",
      "Train Epoch: 13 [13500/19646 (69%)]\tLoss: 0.328115\n",
      "Train Epoch: 13 [14000/19646 (71%)]\tLoss: 0.343978\n",
      "Train Epoch: 13 [14500/19646 (74%)]\tLoss: 0.337508\n",
      "Train Epoch: 13 [15000/19646 (76%)]\tLoss: 0.629766\n",
      "Train Epoch: 13 [15500/19646 (79%)]\tLoss: 0.793316\n",
      "Train Epoch: 13 [16000/19646 (81%)]\tLoss: 0.288859\n",
      "Train Epoch: 13 [16500/19646 (84%)]\tLoss: 0.715026\n",
      "Train Epoch: 13 [17000/19646 (87%)]\tLoss: 0.251370\n",
      "Train Epoch: 13 [17500/19646 (89%)]\tLoss: 0.361856\n",
      "Train Epoch: 13 [18000/19646 (92%)]\tLoss: 0.550413\n",
      "Train Epoch: 13 [18500/19646 (94%)]\tLoss: 0.471963\n",
      "Train Epoch: 13 [19000/19646 (97%)]\tLoss: 0.591717\n",
      "Train Epoch: 13 [19500/19646 (99%)]\tLoss: 0.644531\n",
      "Training Time: 1527.222767829895   Learning Rate: 0.0001\n",
      "Train Epoch: 14 [0/19646 (0%)]\tLoss: 0.614990\n",
      "Train Epoch: 14 [500/19646 (3%)]\tLoss: 0.530631\n",
      "Train Epoch: 14 [1000/19646 (5%)]\tLoss: 0.428628\n",
      "Train Epoch: 14 [1500/19646 (8%)]\tLoss: 0.396666\n",
      "Train Epoch: 14 [2000/19646 (10%)]\tLoss: 0.285032\n",
      "Train Epoch: 14 [2500/19646 (13%)]\tLoss: 0.608323\n",
      "Train Epoch: 14 [3000/19646 (15%)]\tLoss: 0.183936\n",
      "Train Epoch: 14 [3500/19646 (18%)]\tLoss: 0.426734\n",
      "Train Epoch: 14 [4000/19646 (20%)]\tLoss: 0.427770\n",
      "Train Epoch: 14 [4500/19646 (23%)]\tLoss: 0.791920\n",
      "Train Epoch: 14 [5000/19646 (25%)]\tLoss: 0.746935\n",
      "Train Epoch: 14 [5500/19646 (28%)]\tLoss: 0.295130\n",
      "Train Epoch: 14 [6000/19646 (31%)]\tLoss: 0.340801\n",
      "Train Epoch: 14 [6500/19646 (33%)]\tLoss: 0.685138\n",
      "Train Epoch: 14 [7000/19646 (36%)]\tLoss: 0.847045\n",
      "Train Epoch: 14 [7500/19646 (38%)]\tLoss: 0.688783\n",
      "Train Epoch: 14 [8000/19646 (41%)]\tLoss: 1.093955\n",
      "Train Epoch: 14 [8500/19646 (43%)]\tLoss: 0.430702\n",
      "Train Epoch: 14 [9000/19646 (46%)]\tLoss: 0.511488\n",
      "Train Epoch: 14 [9500/19646 (48%)]\tLoss: 0.617342\n",
      "Train Epoch: 14 [10000/19646 (51%)]\tLoss: 0.380078\n",
      "Train Epoch: 14 [10500/19646 (53%)]\tLoss: 0.381059\n",
      "Train Epoch: 14 [11000/19646 (56%)]\tLoss: 0.467817\n",
      "Train Epoch: 14 [11500/19646 (59%)]\tLoss: 0.652225\n",
      "Train Epoch: 14 [12000/19646 (61%)]\tLoss: 0.477732\n",
      "Train Epoch: 14 [12500/19646 (64%)]\tLoss: 0.543989\n",
      "Train Epoch: 14 [13000/19646 (66%)]\tLoss: 0.452995\n",
      "Train Epoch: 14 [13500/19646 (69%)]\tLoss: 0.703352\n",
      "Train Epoch: 14 [14000/19646 (71%)]\tLoss: 0.403304\n",
      "Train Epoch: 14 [14500/19646 (74%)]\tLoss: 0.128348\n",
      "Train Epoch: 14 [15000/19646 (76%)]\tLoss: 0.233376\n",
      "Train Epoch: 14 [15500/19646 (79%)]\tLoss: 0.535250\n",
      "Train Epoch: 14 [16000/19646 (81%)]\tLoss: 0.684055\n",
      "Train Epoch: 14 [16500/19646 (84%)]\tLoss: 0.431791\n",
      "Train Epoch: 14 [17000/19646 (87%)]\tLoss: 1.134190\n",
      "Train Epoch: 14 [17500/19646 (89%)]\tLoss: 0.507254\n",
      "Train Epoch: 14 [18000/19646 (92%)]\tLoss: 0.468883\n",
      "Train Epoch: 14 [18500/19646 (94%)]\tLoss: 0.420642\n",
      "Train Epoch: 14 [19000/19646 (97%)]\tLoss: 1.143054\n",
      "Train Epoch: 14 [19500/19646 (99%)]\tLoss: 0.375978\n",
      "Training Time: 1526.5151770114899   Learning Rate: 0.0001\n",
      "14\n",
      "Test Loss is 0.499923, mean precision is: 30.2207%\n",
      "Train Epoch: 15 [0/19646 (0%)]\tLoss: 0.431153\n",
      "Train Epoch: 15 [500/19646 (3%)]\tLoss: 0.308397\n",
      "Train Epoch: 15 [1000/19646 (5%)]\tLoss: 0.964341\n",
      "Train Epoch: 15 [1500/19646 (8%)]\tLoss: 0.306827\n",
      "Train Epoch: 15 [2000/19646 (10%)]\tLoss: 0.239228\n",
      "Train Epoch: 15 [2500/19646 (13%)]\tLoss: 1.722369\n",
      "Train Epoch: 15 [3000/19646 (15%)]\tLoss: 0.860443\n",
      "Train Epoch: 15 [3500/19646 (18%)]\tLoss: 0.335259\n",
      "Train Epoch: 15 [4000/19646 (20%)]\tLoss: 0.625247\n",
      "Train Epoch: 15 [4500/19646 (23%)]\tLoss: 0.514886\n",
      "Train Epoch: 15 [5000/19646 (25%)]\tLoss: 1.159235\n",
      "Train Epoch: 15 [5500/19646 (28%)]\tLoss: 0.338884\n",
      "Train Epoch: 15 [6000/19646 (31%)]\tLoss: 1.330817\n",
      "Train Epoch: 15 [6500/19646 (33%)]\tLoss: 0.321457\n",
      "Train Epoch: 15 [7000/19646 (36%)]\tLoss: 0.404602\n",
      "Train Epoch: 15 [7500/19646 (38%)]\tLoss: 0.205117\n",
      "Train Epoch: 15 [8000/19646 (41%)]\tLoss: 0.466339\n",
      "Train Epoch: 15 [8500/19646 (43%)]\tLoss: 0.285708\n",
      "Train Epoch: 15 [9000/19646 (46%)]\tLoss: 0.669017\n",
      "Train Epoch: 15 [9500/19646 (48%)]\tLoss: 0.376552\n",
      "Train Epoch: 15 [10000/19646 (51%)]\tLoss: 1.173420\n",
      "Train Epoch: 15 [10500/19646 (53%)]\tLoss: 0.640457\n",
      "Train Epoch: 15 [11000/19646 (56%)]\tLoss: 1.169058\n",
      "Train Epoch: 15 [11500/19646 (59%)]\tLoss: 0.538344\n",
      "Train Epoch: 15 [12000/19646 (61%)]\tLoss: 0.660367\n",
      "Train Epoch: 15 [12500/19646 (64%)]\tLoss: 0.665391\n",
      "Train Epoch: 15 [13000/19646 (66%)]\tLoss: 0.772933\n",
      "Train Epoch: 15 [13500/19646 (69%)]\tLoss: 1.146871\n",
      "Train Epoch: 15 [14000/19646 (71%)]\tLoss: 0.096574\n",
      "Train Epoch: 15 [14500/19646 (74%)]\tLoss: 0.461965\n",
      "Train Epoch: 15 [15000/19646 (76%)]\tLoss: 0.727769\n",
      "Train Epoch: 15 [15500/19646 (79%)]\tLoss: 0.552941\n",
      "Train Epoch: 15 [16000/19646 (81%)]\tLoss: 0.731430\n",
      "Train Epoch: 15 [16500/19646 (84%)]\tLoss: 0.252828\n",
      "Train Epoch: 15 [17000/19646 (87%)]\tLoss: 0.234913\n",
      "Train Epoch: 15 [17500/19646 (89%)]\tLoss: 0.428067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [18000/19646 (92%)]\tLoss: 0.766528\n",
      "Train Epoch: 15 [18500/19646 (94%)]\tLoss: 0.826172\n",
      "Train Epoch: 15 [19000/19646 (97%)]\tLoss: 0.264174\n",
      "Train Epoch: 15 [19500/19646 (99%)]\tLoss: 1.291928\n",
      "Training Time: 1526.9862623214722   Learning Rate: 0.0001\n",
      "Train Epoch: 16 [0/19646 (0%)]\tLoss: 0.875346\n",
      "Train Epoch: 16 [500/19646 (3%)]\tLoss: 0.658256\n",
      "Train Epoch: 16 [1000/19646 (5%)]\tLoss: 0.340047\n",
      "Train Epoch: 16 [1500/19646 (8%)]\tLoss: 0.447941\n",
      "Train Epoch: 16 [2000/19646 (10%)]\tLoss: 0.568456\n",
      "Train Epoch: 16 [2500/19646 (13%)]\tLoss: 0.804743\n",
      "Train Epoch: 16 [3000/19646 (15%)]\tLoss: 0.782440\n",
      "Train Epoch: 16 [3500/19646 (18%)]\tLoss: 0.145771\n",
      "Train Epoch: 16 [4000/19646 (20%)]\tLoss: 0.339800\n",
      "Train Epoch: 16 [4500/19646 (23%)]\tLoss: 1.121170\n",
      "Train Epoch: 16 [5000/19646 (25%)]\tLoss: 0.489388\n",
      "Train Epoch: 16 [5500/19646 (28%)]\tLoss: 0.677316\n",
      "Train Epoch: 16 [6000/19646 (31%)]\tLoss: 0.490867\n",
      "Train Epoch: 16 [6500/19646 (33%)]\tLoss: 1.145512\n",
      "Train Epoch: 16 [7000/19646 (36%)]\tLoss: 0.376725\n",
      "Train Epoch: 16 [7500/19646 (38%)]\tLoss: 0.535601\n",
      "Train Epoch: 16 [8000/19646 (41%)]\tLoss: 0.640298\n",
      "Train Epoch: 16 [8500/19646 (43%)]\tLoss: 0.946966\n",
      "Train Epoch: 16 [9000/19646 (46%)]\tLoss: 0.397465\n",
      "Train Epoch: 16 [9500/19646 (48%)]\tLoss: 0.343833\n",
      "Train Epoch: 16 [10000/19646 (51%)]\tLoss: 0.283706\n",
      "Train Epoch: 16 [10500/19646 (53%)]\tLoss: 0.088756\n",
      "Train Epoch: 16 [11000/19646 (56%)]\tLoss: 0.746748\n",
      "Train Epoch: 16 [11500/19646 (59%)]\tLoss: 0.283288\n",
      "Train Epoch: 16 [12000/19646 (61%)]\tLoss: 0.626984\n",
      "Train Epoch: 16 [12500/19646 (64%)]\tLoss: 1.270501\n",
      "Train Epoch: 16 [13000/19646 (66%)]\tLoss: 0.792875\n",
      "Train Epoch: 16 [13500/19646 (69%)]\tLoss: 1.091373\n",
      "Train Epoch: 16 [14000/19646 (71%)]\tLoss: 0.283106\n",
      "Train Epoch: 16 [14500/19646 (74%)]\tLoss: 1.286902\n",
      "Train Epoch: 16 [15000/19646 (76%)]\tLoss: 0.401324\n",
      "Train Epoch: 16 [15500/19646 (79%)]\tLoss: 0.401846\n",
      "Train Epoch: 16 [16000/19646 (81%)]\tLoss: 1.182426\n",
      "Train Epoch: 16 [16500/19646 (84%)]\tLoss: 0.455693\n",
      "Train Epoch: 16 [17000/19646 (87%)]\tLoss: 0.198235\n",
      "Train Epoch: 16 [17500/19646 (89%)]\tLoss: 0.114522\n",
      "Train Epoch: 16 [18000/19646 (92%)]\tLoss: 0.207317\n",
      "Train Epoch: 16 [18500/19646 (94%)]\tLoss: 0.509966\n",
      "Train Epoch: 16 [19000/19646 (97%)]\tLoss: 0.267934\n",
      "Train Epoch: 16 [19500/19646 (99%)]\tLoss: 0.517504\n",
      "Training Time: 1522.5578649044037   Learning Rate: 1e-05\n",
      "16\n",
      "Test Loss is 0.503989, mean precision is: 28.8139%\n",
      "Train Epoch: 17 [0/19646 (0%)]\tLoss: 0.348774\n",
      "Train Epoch: 17 [500/19646 (3%)]\tLoss: 0.487948\n",
      "Train Epoch: 17 [1000/19646 (5%)]\tLoss: 0.522924\n",
      "Train Epoch: 17 [1500/19646 (8%)]\tLoss: 0.381248\n",
      "Train Epoch: 17 [2000/19646 (10%)]\tLoss: 0.413838\n",
      "Train Epoch: 17 [2500/19646 (13%)]\tLoss: 0.483837\n",
      "Train Epoch: 17 [3000/19646 (15%)]\tLoss: 0.568704\n",
      "Train Epoch: 17 [3500/19646 (18%)]\tLoss: 0.162379\n",
      "Train Epoch: 17 [4000/19646 (20%)]\tLoss: 1.036861\n",
      "Train Epoch: 17 [4500/19646 (23%)]\tLoss: 0.854947\n",
      "Train Epoch: 17 [5000/19646 (25%)]\tLoss: 0.953529\n",
      "Train Epoch: 17 [5500/19646 (28%)]\tLoss: 0.340211\n",
      "Train Epoch: 17 [6000/19646 (31%)]\tLoss: 0.502157\n",
      "Train Epoch: 17 [6500/19646 (33%)]\tLoss: 0.298559\n",
      "Train Epoch: 17 [7000/19646 (36%)]\tLoss: 0.113080\n",
      "Train Epoch: 17 [7500/19646 (38%)]\tLoss: 0.522624\n",
      "Train Epoch: 17 [8000/19646 (41%)]\tLoss: 0.607440\n",
      "Train Epoch: 17 [8500/19646 (43%)]\tLoss: 0.398127\n",
      "Train Epoch: 17 [9000/19646 (46%)]\tLoss: 0.328346\n",
      "Train Epoch: 17 [9500/19646 (48%)]\tLoss: 0.635568\n",
      "Train Epoch: 17 [10000/19646 (51%)]\tLoss: 0.396610\n",
      "Train Epoch: 17 [10500/19646 (53%)]\tLoss: 0.389468\n",
      "Train Epoch: 17 [11000/19646 (56%)]\tLoss: 0.378799\n",
      "Train Epoch: 17 [11500/19646 (59%)]\tLoss: 0.126374\n",
      "Train Epoch: 17 [12000/19646 (61%)]\tLoss: 0.554204\n",
      "Train Epoch: 17 [12500/19646 (64%)]\tLoss: 0.401198\n",
      "Train Epoch: 17 [13000/19646 (66%)]\tLoss: 0.867002\n",
      "Train Epoch: 17 [13500/19646 (69%)]\tLoss: 0.134552\n",
      "Train Epoch: 17 [14000/19646 (71%)]\tLoss: 0.336831\n",
      "Train Epoch: 17 [14500/19646 (74%)]\tLoss: 0.458648\n",
      "Train Epoch: 17 [15000/19646 (76%)]\tLoss: 0.435346\n",
      "Train Epoch: 17 [15500/19646 (79%)]\tLoss: 0.399618\n",
      "Train Epoch: 17 [16000/19646 (81%)]\tLoss: 0.775691\n",
      "Train Epoch: 17 [16500/19646 (84%)]\tLoss: 0.823191\n",
      "Train Epoch: 17 [17000/19646 (87%)]\tLoss: 0.185455\n",
      "Train Epoch: 17 [17500/19646 (89%)]\tLoss: 0.342714\n",
      "Train Epoch: 17 [18000/19646 (92%)]\tLoss: 0.322855\n",
      "Train Epoch: 17 [18500/19646 (94%)]\tLoss: 0.642854\n",
      "Train Epoch: 17 [19000/19646 (97%)]\tLoss: 0.424902\n",
      "Train Epoch: 17 [19500/19646 (99%)]\tLoss: 0.106648\n",
      "Training Time: 1514.267472743988   Learning Rate: 1e-05\n",
      "Train Epoch: 18 [0/19646 (0%)]\tLoss: 0.278610\n",
      "Train Epoch: 18 [500/19646 (3%)]\tLoss: 0.177671\n",
      "Train Epoch: 18 [1000/19646 (5%)]\tLoss: 0.841028\n",
      "Train Epoch: 18 [1500/19646 (8%)]\tLoss: 0.794848\n",
      "Train Epoch: 18 [2000/19646 (10%)]\tLoss: 0.709718\n",
      "Train Epoch: 18 [2500/19646 (13%)]\tLoss: 0.189662\n",
      "Train Epoch: 18 [3000/19646 (15%)]\tLoss: 0.723678\n",
      "Train Epoch: 18 [3500/19646 (18%)]\tLoss: 0.838492\n",
      "Train Epoch: 18 [4000/19646 (20%)]\tLoss: 0.731736\n",
      "Train Epoch: 18 [4500/19646 (23%)]\tLoss: 0.557188\n",
      "Train Epoch: 18 [5000/19646 (25%)]\tLoss: 0.350771\n",
      "Train Epoch: 18 [5500/19646 (28%)]\tLoss: 0.560910\n",
      "Train Epoch: 18 [6000/19646 (31%)]\tLoss: 0.366851\n",
      "Train Epoch: 18 [6500/19646 (33%)]\tLoss: 1.030019\n",
      "Train Epoch: 18 [7000/19646 (36%)]\tLoss: 0.343802\n",
      "Train Epoch: 18 [7500/19646 (38%)]\tLoss: 0.794771\n",
      "Train Epoch: 18 [8000/19646 (41%)]\tLoss: 0.208785\n",
      "Train Epoch: 18 [8500/19646 (43%)]\tLoss: 0.329394\n",
      "Train Epoch: 18 [9000/19646 (46%)]\tLoss: 1.090179\n",
      "Train Epoch: 18 [9500/19646 (48%)]\tLoss: 0.331580\n",
      "Train Epoch: 18 [10000/19646 (51%)]\tLoss: 0.210018\n",
      "Train Epoch: 18 [10500/19646 (53%)]\tLoss: 0.477890\n",
      "Train Epoch: 18 [11000/19646 (56%)]\tLoss: 0.294688\n",
      "Train Epoch: 18 [11500/19646 (59%)]\tLoss: 1.134111\n",
      "Train Epoch: 18 [12000/19646 (61%)]\tLoss: 1.309731\n",
      "Train Epoch: 18 [12500/19646 (64%)]\tLoss: 0.266377\n",
      "Train Epoch: 18 [13000/19646 (66%)]\tLoss: 0.220481\n",
      "Train Epoch: 18 [13500/19646 (69%)]\tLoss: 0.567151\n",
      "Train Epoch: 18 [14000/19646 (71%)]\tLoss: 0.244458\n",
      "Train Epoch: 18 [14500/19646 (74%)]\tLoss: 0.565651\n",
      "Train Epoch: 18 [15000/19646 (76%)]\tLoss: 0.085850\n",
      "Train Epoch: 18 [15500/19646 (79%)]\tLoss: 0.214913\n",
      "Train Epoch: 18 [16000/19646 (81%)]\tLoss: 0.262119\n",
      "Train Epoch: 18 [16500/19646 (84%)]\tLoss: 0.401423\n",
      "Train Epoch: 18 [17000/19646 (87%)]\tLoss: 0.441188\n",
      "Train Epoch: 18 [17500/19646 (89%)]\tLoss: 0.494358\n",
      "Train Epoch: 18 [18000/19646 (92%)]\tLoss: 0.177398\n",
      "Train Epoch: 18 [18500/19646 (94%)]\tLoss: 0.515526\n",
      "Train Epoch: 18 [19000/19646 (97%)]\tLoss: 0.524738\n",
      "Train Epoch: 18 [19500/19646 (99%)]\tLoss: 0.584875\n",
      "Training Time: 1520.6407804489136   Learning Rate: 1e-05\n",
      "18\n",
      "Test Loss is 0.474929, mean precision is: 29.6358%\n",
      "Train Epoch: 19 [0/19646 (0%)]\tLoss: 1.075864\n",
      "Train Epoch: 19 [500/19646 (3%)]\tLoss: 1.604691\n",
      "Train Epoch: 19 [1000/19646 (5%)]\tLoss: 0.335818\n",
      "Train Epoch: 19 [1500/19646 (8%)]\tLoss: 0.208962\n",
      "Train Epoch: 19 [2000/19646 (10%)]\tLoss: 0.200865\n",
      "Train Epoch: 19 [2500/19646 (13%)]\tLoss: 0.390917\n",
      "Train Epoch: 19 [3000/19646 (15%)]\tLoss: 0.322945\n",
      "Train Epoch: 19 [3500/19646 (18%)]\tLoss: 0.507720\n",
      "Train Epoch: 19 [4000/19646 (20%)]\tLoss: 0.222410\n",
      "Train Epoch: 19 [4500/19646 (23%)]\tLoss: 0.222061\n",
      "Train Epoch: 19 [5000/19646 (25%)]\tLoss: 0.733378\n",
      "Train Epoch: 19 [5500/19646 (28%)]\tLoss: 0.146964\n",
      "Train Epoch: 19 [6000/19646 (31%)]\tLoss: 0.236453\n",
      "Train Epoch: 19 [6500/19646 (33%)]\tLoss: 0.559536\n",
      "Train Epoch: 19 [7000/19646 (36%)]\tLoss: 0.341925\n",
      "Train Epoch: 19 [7500/19646 (38%)]\tLoss: 0.411358\n",
      "Train Epoch: 19 [8000/19646 (41%)]\tLoss: 1.068221\n",
      "Train Epoch: 19 [8500/19646 (43%)]\tLoss: 0.287119\n",
      "Train Epoch: 19 [9000/19646 (46%)]\tLoss: 0.134797\n",
      "Train Epoch: 19 [9500/19646 (48%)]\tLoss: 0.360990\n",
      "Train Epoch: 19 [10000/19646 (51%)]\tLoss: 0.321288\n",
      "Train Epoch: 19 [10500/19646 (53%)]\tLoss: 0.572174\n",
      "Train Epoch: 19 [11000/19646 (56%)]\tLoss: 0.220483\n",
      "Train Epoch: 19 [11500/19646 (59%)]\tLoss: 0.430069\n",
      "Train Epoch: 19 [12000/19646 (61%)]\tLoss: 0.246318\n",
      "Train Epoch: 19 [12500/19646 (64%)]\tLoss: 0.549616\n",
      "Train Epoch: 19 [13000/19646 (66%)]\tLoss: 0.546036\n",
      "Train Epoch: 19 [13500/19646 (69%)]\tLoss: 0.596193\n",
      "Train Epoch: 19 [14000/19646 (71%)]\tLoss: 0.593624\n",
      "Train Epoch: 19 [14500/19646 (74%)]\tLoss: 0.504594\n",
      "Train Epoch: 19 [15000/19646 (76%)]\tLoss: 0.953742\n",
      "Train Epoch: 19 [15500/19646 (79%)]\tLoss: 0.407408\n",
      "Train Epoch: 19 [16000/19646 (81%)]\tLoss: 0.466947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 19 [16500/19646 (84%)]\tLoss: 0.499153\n",
      "Train Epoch: 19 [17000/19646 (87%)]\tLoss: 0.896169\n",
      "Train Epoch: 19 [17500/19646 (89%)]\tLoss: 0.164158\n",
      "Train Epoch: 19 [18000/19646 (92%)]\tLoss: 0.372301\n",
      "Train Epoch: 19 [18500/19646 (94%)]\tLoss: 0.390201\n",
      "Train Epoch: 19 [19000/19646 (97%)]\tLoss: 0.222657\n",
      "Train Epoch: 19 [19500/19646 (99%)]\tLoss: 0.521748\n",
      "Training Time: 1519.6876022815704   Learning Rate: 1e-05\n",
      "Train Epoch: 20 [0/19646 (0%)]\tLoss: 0.105538\n",
      "Train Epoch: 20 [500/19646 (3%)]\tLoss: 0.797773\n",
      "Train Epoch: 20 [1000/19646 (5%)]\tLoss: 0.495646\n",
      "Train Epoch: 20 [1500/19646 (8%)]\tLoss: 0.708907\n",
      "Train Epoch: 20 [2000/19646 (10%)]\tLoss: 0.247101\n",
      "Train Epoch: 20 [2500/19646 (13%)]\tLoss: 0.425348\n",
      "Train Epoch: 20 [3000/19646 (15%)]\tLoss: 0.478533\n",
      "Train Epoch: 20 [3500/19646 (18%)]\tLoss: 0.595867\n",
      "Train Epoch: 20 [4000/19646 (20%)]\tLoss: 0.294470\n",
      "Train Epoch: 20 [4500/19646 (23%)]\tLoss: 0.142017\n",
      "Train Epoch: 20 [5000/19646 (25%)]\tLoss: 0.588801\n",
      "Train Epoch: 20 [5500/19646 (28%)]\tLoss: 0.344738\n",
      "Train Epoch: 20 [6000/19646 (31%)]\tLoss: 0.529334\n",
      "Train Epoch: 20 [6500/19646 (33%)]\tLoss: 0.442596\n",
      "Train Epoch: 20 [7000/19646 (36%)]\tLoss: 0.850481\n",
      "Train Epoch: 20 [7500/19646 (38%)]\tLoss: 0.628841\n",
      "Train Epoch: 20 [8000/19646 (41%)]\tLoss: 0.442074\n",
      "Train Epoch: 20 [8500/19646 (43%)]\tLoss: 0.371478\n",
      "Train Epoch: 20 [9000/19646 (46%)]\tLoss: 0.383460\n",
      "Train Epoch: 20 [9500/19646 (48%)]\tLoss: 0.595712\n",
      "Train Epoch: 20 [10000/19646 (51%)]\tLoss: 0.587291\n",
      "Train Epoch: 20 [10500/19646 (53%)]\tLoss: 0.358630\n",
      "Train Epoch: 20 [11000/19646 (56%)]\tLoss: 1.422756\n",
      "Train Epoch: 20 [11500/19646 (59%)]\tLoss: 0.640157\n",
      "Train Epoch: 20 [12000/19646 (61%)]\tLoss: 0.132835\n",
      "Train Epoch: 20 [12500/19646 (64%)]\tLoss: 0.199318\n",
      "Train Epoch: 20 [13000/19646 (66%)]\tLoss: 0.261262\n",
      "Train Epoch: 20 [13500/19646 (69%)]\tLoss: 0.469581\n",
      "Train Epoch: 20 [14000/19646 (71%)]\tLoss: 0.627358\n",
      "Train Epoch: 20 [14500/19646 (74%)]\tLoss: 0.151140\n",
      "Train Epoch: 20 [15000/19646 (76%)]\tLoss: 0.250920\n",
      "Train Epoch: 20 [15500/19646 (79%)]\tLoss: 0.750497\n",
      "Train Epoch: 20 [16000/19646 (81%)]\tLoss: 0.393209\n",
      "Train Epoch: 20 [16500/19646 (84%)]\tLoss: 0.870947\n",
      "Train Epoch: 20 [17000/19646 (87%)]\tLoss: 0.439140\n",
      "Train Epoch: 20 [17500/19646 (89%)]\tLoss: 0.174767\n",
      "Train Epoch: 20 [18000/19646 (92%)]\tLoss: 1.610113\n",
      "Train Epoch: 20 [18500/19646 (94%)]\tLoss: 1.701039\n",
      "Train Epoch: 20 [19000/19646 (97%)]\tLoss: 0.579047\n",
      "Train Epoch: 20 [19500/19646 (99%)]\tLoss: 0.231745\n",
      "Training Time: 1521.0031623840332   Learning Rate: 1e-05\n",
      "20\n",
      "Test Loss is 0.483909, mean precision is: 30.0569%\n",
      "Train Epoch: 21 [0/19646 (0%)]\tLoss: 0.368001\n",
      "Train Epoch: 21 [500/19646 (3%)]\tLoss: 0.442349\n",
      "Train Epoch: 21 [1000/19646 (5%)]\tLoss: 0.367785\n",
      "Train Epoch: 21 [1500/19646 (8%)]\tLoss: 0.566841\n",
      "Train Epoch: 21 [2000/19646 (10%)]\tLoss: 0.299082\n",
      "Train Epoch: 21 [2500/19646 (13%)]\tLoss: 0.747860\n",
      "Train Epoch: 21 [3000/19646 (15%)]\tLoss: 0.652868\n",
      "Train Epoch: 21 [3500/19646 (18%)]\tLoss: 0.220507\n",
      "Train Epoch: 21 [4000/19646 (20%)]\tLoss: 0.429739\n",
      "Train Epoch: 21 [4500/19646 (23%)]\tLoss: 0.341937\n",
      "Train Epoch: 21 [5000/19646 (25%)]\tLoss: 0.256792\n",
      "Train Epoch: 21 [5500/19646 (28%)]\tLoss: 0.312089\n",
      "Train Epoch: 21 [6000/19646 (31%)]\tLoss: 0.303481\n",
      "Train Epoch: 21 [6500/19646 (33%)]\tLoss: 0.578186\n",
      "Train Epoch: 21 [7000/19646 (36%)]\tLoss: 0.389913\n",
      "Train Epoch: 21 [7500/19646 (38%)]\tLoss: 0.471600\n",
      "Train Epoch: 21 [8000/19646 (41%)]\tLoss: 0.476955\n",
      "Train Epoch: 21 [8500/19646 (43%)]\tLoss: 0.930311\n",
      "Train Epoch: 21 [9000/19646 (46%)]\tLoss: 0.175277\n",
      "Train Epoch: 21 [9500/19646 (48%)]\tLoss: 1.041164\n",
      "Train Epoch: 21 [10000/19646 (51%)]\tLoss: 0.571175\n",
      "Train Epoch: 21 [10500/19646 (53%)]\tLoss: 0.197514\n",
      "Train Epoch: 21 [11000/19646 (56%)]\tLoss: 0.501157\n",
      "Train Epoch: 21 [11500/19646 (59%)]\tLoss: 0.110444\n",
      "Train Epoch: 21 [12000/19646 (61%)]\tLoss: 0.248803\n",
      "Train Epoch: 21 [12500/19646 (64%)]\tLoss: 0.753803\n",
      "Train Epoch: 21 [13000/19646 (66%)]\tLoss: 0.745579\n",
      "Train Epoch: 21 [13500/19646 (69%)]\tLoss: 0.236823\n",
      "Train Epoch: 21 [14000/19646 (71%)]\tLoss: 0.837823\n",
      "Train Epoch: 21 [14500/19646 (74%)]\tLoss: 0.504941\n",
      "Train Epoch: 21 [15000/19646 (76%)]\tLoss: 0.815736\n",
      "Train Epoch: 21 [15500/19646 (79%)]\tLoss: 0.313240\n",
      "Train Epoch: 21 [16000/19646 (81%)]\tLoss: 0.366021\n",
      "Train Epoch: 21 [16500/19646 (84%)]\tLoss: 0.426841\n",
      "Train Epoch: 21 [17000/19646 (87%)]\tLoss: 0.125422\n",
      "Train Epoch: 21 [17500/19646 (89%)]\tLoss: 0.444081\n",
      "Train Epoch: 21 [18000/19646 (92%)]\tLoss: 0.242870\n",
      "Train Epoch: 21 [18500/19646 (94%)]\tLoss: 0.483109\n",
      "Train Epoch: 21 [19000/19646 (97%)]\tLoss: 0.213414\n",
      "Train Epoch: 21 [19500/19646 (99%)]\tLoss: 0.488422\n",
      "Training Time: 1519.031631231308   Learning Rate: 1e-05\n",
      "Train Epoch: 22 [0/19646 (0%)]\tLoss: 0.841446\n",
      "Train Epoch: 22 [500/19646 (3%)]\tLoss: 0.359197\n",
      "Train Epoch: 22 [1000/19646 (5%)]\tLoss: 0.602868\n",
      "Train Epoch: 22 [1500/19646 (8%)]\tLoss: 0.281915\n",
      "Train Epoch: 22 [2000/19646 (10%)]\tLoss: 0.213950\n",
      "Train Epoch: 22 [2500/19646 (13%)]\tLoss: 0.608577\n",
      "Train Epoch: 22 [3000/19646 (15%)]\tLoss: 0.484086\n",
      "Train Epoch: 22 [3500/19646 (18%)]\tLoss: 0.628862\n",
      "Train Epoch: 22 [4000/19646 (20%)]\tLoss: 0.286530\n",
      "Train Epoch: 22 [4500/19646 (23%)]\tLoss: 0.511144\n",
      "Train Epoch: 22 [5000/19646 (25%)]\tLoss: 0.790936\n",
      "Train Epoch: 22 [5500/19646 (28%)]\tLoss: 0.331560\n",
      "Train Epoch: 22 [6000/19646 (31%)]\tLoss: 0.460847\n",
      "Train Epoch: 22 [6500/19646 (33%)]\tLoss: 0.308973\n",
      "Train Epoch: 22 [7000/19646 (36%)]\tLoss: 0.340010\n",
      "Train Epoch: 22 [7500/19646 (38%)]\tLoss: 0.227515\n",
      "Train Epoch: 22 [8000/19646 (41%)]\tLoss: 0.786972\n",
      "Train Epoch: 22 [8500/19646 (43%)]\tLoss: 0.503797\n",
      "Train Epoch: 22 [9000/19646 (46%)]\tLoss: 0.656322\n",
      "Train Epoch: 22 [9500/19646 (48%)]\tLoss: 0.677291\n",
      "Train Epoch: 22 [10000/19646 (51%)]\tLoss: 0.292926\n",
      "Train Epoch: 22 [10500/19646 (53%)]\tLoss: 0.360976\n",
      "Train Epoch: 22 [11000/19646 (56%)]\tLoss: 0.982131\n",
      "Train Epoch: 22 [11500/19646 (59%)]\tLoss: 0.287763\n",
      "Train Epoch: 22 [12000/19646 (61%)]\tLoss: 0.296286\n",
      "Train Epoch: 22 [12500/19646 (64%)]\tLoss: 0.396299\n",
      "Train Epoch: 22 [13000/19646 (66%)]\tLoss: 0.291385\n",
      "Train Epoch: 22 [13500/19646 (69%)]\tLoss: 0.482180\n",
      "Train Epoch: 22 [14000/19646 (71%)]\tLoss: 0.286205\n",
      "Train Epoch: 22 [14500/19646 (74%)]\tLoss: 0.274109\n",
      "Train Epoch: 22 [15000/19646 (76%)]\tLoss: 0.340411\n",
      "Train Epoch: 22 [15500/19646 (79%)]\tLoss: 0.542568\n",
      "Train Epoch: 22 [16000/19646 (81%)]\tLoss: 0.618323\n",
      "Train Epoch: 22 [16500/19646 (84%)]\tLoss: 0.376051\n",
      "Train Epoch: 22 [17000/19646 (87%)]\tLoss: 0.375370\n",
      "Train Epoch: 22 [17500/19646 (89%)]\tLoss: 0.163864\n",
      "Train Epoch: 22 [18000/19646 (92%)]\tLoss: 0.885355\n",
      "Train Epoch: 22 [18500/19646 (94%)]\tLoss: 0.832833\n",
      "Train Epoch: 22 [19000/19646 (97%)]\tLoss: 0.422034\n",
      "Train Epoch: 22 [19500/19646 (99%)]\tLoss: 0.239718\n",
      "Training Time: 1518.4284217357635   Learning Rate: 1e-05\n",
      "22\n",
      "Test Loss is 0.492249, mean precision is: 29.6335%\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/makisechris/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py:216: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.\n",
      "  warnings.warn(\"NLLLoss2d has been deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss is 1.629143, mean precision is: 0.8994%\n",
      "27\n",
      "Test Loss is 1.519135, mean precision is: 2.3174%\n",
      "30\n",
      "Test Loss is 1.602930, mean precision is: 1.3016%\n",
      "33\n",
      "Test Loss is 1.439184, mean precision is: 1.9290%\n",
      "36\n",
      "Test Loss is 1.616317, mean precision is: 1.4456%\n",
      "39\n",
      "Test Loss is 1.692963, mean precision is: 1.5148%\n",
      "42\n",
      "Test Loss is 1.623588, mean precision is: 2.0390%\n",
      "45\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4fbc7d4e1669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tmp/model{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFarmDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mistrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misaug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-963d96130c1d>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, device, testdataset, issave)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1472\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1472\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jingwei Algriculture AI competition/farmdataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#targetimg=transforms.ToTensor()(targetimg).squeeze(0).long()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mtargetimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mtargetimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m#to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;31m#print(sampleimg.shape,targetimg.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msampleimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargetimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, and uint8."
     ]
    }
   ],
   "source": [
    "for epoch in range(24, 69, 3):\n",
    "    print(epoch)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = torch.load('./tmp/model{}'.format(epoch))\n",
    "    test(model, device, FarmDataset(istrain = True, isaug = False), issave = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
